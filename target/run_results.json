{"metadata": {"dbt_schema_version": "https://schemas.getdbt.com/dbt/run-results/v4.json", "dbt_version": "1.0.5", "generated_at": "2022-06-27T17:36:29.278239Z", "invocation_id": "779f3bd9-1e92-4d0b-a20d-704360cd7056", "env": {}}, "results": [{"status": "error", "timing": [], "thread_id": "Thread-1", "execution_time": 18.44457769393921, "adapter_response": {}, "message": "Runtime Error in model 4599_system_note (models\\example\\4599_system_note.sql)\n  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: netsuite_suiteanalytics.4599_system_note; line 6 pos 34;\n  'GlobalLimit 10\n  +- 'LocalLimit 10\n     +- 'Project ['action, 'audit_file]\n        +- 'UnresolvedRelation [netsuite_suiteanalytics, 4599_system_note], [], false\n  \n  \tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)\n  \tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)\n  \tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n  \tat org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)\n  \tat org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)\n  \tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)\n  \tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)\n  \tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)\n  \tat java.security.AccessController.doPrivileged(Native Method)\n  \tat javax.security.auth.Subject.doAs(Subject.java:422)\n  \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)\n  \tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)\n  \tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n  \tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n  \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n  \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n  \tat java.lang.Thread.run(Thread.java:748)\n  Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: netsuite_suiteanalytics.4599_system_note; line 6 pos 34;\n  'GlobalLimit 10\n  +- 'LocalLimit 10\n     +- 'Project ['action, 'audit_file]\n        +- 'UnresolvedRelation [netsuite_suiteanalytics, 4599_system_note], [], false\n  \n  \tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n  \tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)\n  \tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)\n  \tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)\n  \tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)\n  \tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)\n  \tat scala.collection.Iterator.foreach(Iterator.scala:941)\n  \tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n  \tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n  \tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n  \tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n  \tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n  \tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)\n  \tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)\n  \tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)\n  \tat scala.collection.Iterator.foreach(Iterator.scala:941)\n  \tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n  \tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n  \tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n  \tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n  \tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n  \tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)\n  \tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)\n  \tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)\n  \tat scala.collection.Iterator.foreach(Iterator.scala:941)\n  \tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n  \tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n  \tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n  \tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n  \tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n  \tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)\n  \tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)\n  \tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n  \tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n  \tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)\n  \tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)\n  \tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)\n  \tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)\n  \tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)\n  \tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)\n  \tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)\n  \tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n  \tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)\n  \tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)\n  \tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n  \tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)\n  \tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)\n  \tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)\n  \tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)\n  \tat org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)\n  \tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n  \tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n  \tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n  \tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)\n  \tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)\n  \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n  \tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n  \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n  \tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n  \tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n  \tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n  \tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)\n  \tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)\n  \tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)\n  \tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n  \tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)\n  \tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)\n  \tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)\n  \tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)\n  \t... 16 more\n  ", "failures": null, "unique_id": "model.dbt_fivetran_1.4599_system_note"}], "elapsed_time": 29.254112243652344, "args": {"write_json": true, "use_colors": true, "printer_width": 80, "version_check": true, "partial_parse": true, "static_parser": true, "profiles_dir": "C:\\Users\\DELL\\.dbt", "send_anonymous_usage_stats": true, "event_buffer_size": 100000, "select": ["example.4599_system_note"], "which": "run", "rpc_method": "run", "indirect_selection": "eager"}}