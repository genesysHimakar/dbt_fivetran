

============================== 2022-06-16 14:15:55.338718 | 928bd20e-abf9-47ba-afe5-985d0640b7cd ==============================
14:15:55.338718 [info ] [MainThread]: Running with dbt=1.0.5
14:15:55.339720 [debug] [MainThread]: running dbt with arguments Namespace(cls=<class 'dbt.task.debug.DebugTask'>, config_dir=False, debug=None, defer=None, event_buffer_size=None, fail_fast=None, log_cache_events=False, log_format=None, partial_parse=None, printer_width=None, profile=None, profiles_dir='C:\\Users\\DELL\\.dbt', project_dir=None, record_timing_info=None, rpc_method=None, send_anonymous_usage_stats=None, single_threaded=False, state=None, static_parser=None, target=None, use_colors=None, use_experimental_parser=None, vars='{}', version_check=None, warn_error=None, which='debug', write_json=None)
14:15:55.378719 [debug] [MainThread]: Tracking: tracking
14:15:55.379673 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023BAFC6ACA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023BAFC93E50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023BAFC93E80>]}
14:15:56.305669 [debug] [MainThread]: Executing "git --help"
14:15:56.453628 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
14:15:56.454628 [debug] [MainThread]: STDERR: "b''"
14:15:56.459621 [debug] [MainThread]: Acquiring new databricks connection "debug"
14:15:56.459621 [debug] [MainThread]: Using databricks connection "debug"
14:15:56.460624 [debug] [MainThread]: On debug: select 1 as id
14:15:56.460624 [debug] [MainThread]: Opening a new connection, currently in state init
14:19:14.846860 [debug] [MainThread]: SQL status: OK in 198.39 seconds
14:19:14.847861 [debug] [MainThread]: On debug: Close
14:19:25.385328 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023BB83E3790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023BB83E3610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023BB83E3FA0>]}
14:19:26.322675 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2022-06-17 12:03:27.458269 | 1ba2f920-86da-4718-848f-9573525ecf02 ==============================
12:03:27.458269 [info ] [MainThread]: Running with dbt=1.0.5
12:03:27.460272 [debug] [MainThread]: running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=None, defer=None, event_buffer_size=None, exclude=None, fail_fast=None, full_refresh=False, log_cache_events=False, log_format=None, partial_parse=None, printer_width=None, profile=None, profiles_dir='C:\\Users\\DELL\\.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=['example.100kusd'], selector_name=None, send_anonymous_usage_stats=None, single_threaded=False, state=None, static_parser=None, target=None, threads=None, use_colors=None, use_experimental_parser=None, vars='{}', version_check=None, warn_error=None, which='run', write_json=None)
12:03:27.461270 [debug] [MainThread]: Tracking: tracking
12:03:27.464267 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002439D012CA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002439D012D30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002439D012D60>]}
12:03:27.514267 [info ] [MainThread]: Partial parse save file not found. Starting full parse.
12:03:27.515269 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '1ba2f920-86da-4718-848f-9573525ecf02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002439D03F8B0>]}
12:03:27.657267 [debug] [MainThread]: Parsing macros\adapters.sql
12:03:27.715270 [debug] [MainThread]: Parsing macros\materializations\seed.sql
12:03:27.728267 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
12:03:27.764272 [debug] [MainThread]: Parsing macros\materializations\table.sql
12:03:27.769272 [debug] [MainThread]: Parsing macros\materializations\view.sql
12:03:27.770273 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
12:03:27.785268 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
12:03:27.790266 [debug] [MainThread]: Parsing macros\materializations\incremental\validate.sql
12:03:27.801270 [debug] [MainThread]: Parsing macros\adapters.sql
12:03:27.894264 [debug] [MainThread]: Parsing macros\materializations\seed.sql
12:03:27.914270 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
12:03:27.962275 [debug] [MainThread]: Parsing macros\materializations\table.sql
12:03:27.969267 [debug] [MainThread]: Parsing macros\materializations\view.sql
12:03:27.970269 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
12:03:27.983270 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
12:03:27.995267 [debug] [MainThread]: Parsing macros\materializations\incremental\validate.sql
12:03:28.010269 [debug] [MainThread]: Parsing macros\adapters\columns.sql
12:03:28.040272 [debug] [MainThread]: Parsing macros\adapters\freshness.sql
12:03:28.047271 [debug] [MainThread]: Parsing macros\adapters\indexes.sql
12:03:28.052272 [debug] [MainThread]: Parsing macros\adapters\metadata.sql
12:03:28.065268 [debug] [MainThread]: Parsing macros\adapters\persist_docs.sql
12:03:28.075273 [debug] [MainThread]: Parsing macros\adapters\relation.sql
12:03:28.092268 [debug] [MainThread]: Parsing macros\adapters\schema.sql
12:03:28.095269 [debug] [MainThread]: Parsing macros\etc\datetime.sql
12:03:28.115269 [debug] [MainThread]: Parsing macros\etc\statement.sql
12:03:28.124268 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
12:03:28.127277 [debug] [MainThread]: Parsing macros\generic_test_sql\not_null.sql
12:03:28.128274 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
12:03:28.130269 [debug] [MainThread]: Parsing macros\generic_test_sql\unique.sql
12:03:28.131269 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_alias.sql
12:03:28.134269 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_database.sql
12:03:28.137269 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_schema.sql
12:03:28.141267 [debug] [MainThread]: Parsing macros\materializations\configs.sql
12:03:28.145267 [debug] [MainThread]: Parsing macros\materializations\hooks.sql
12:03:28.153267 [debug] [MainThread]: Parsing macros\materializations\models\incremental\column_helpers.sql
12:03:28.160267 [debug] [MainThread]: Parsing macros\materializations\models\incremental\incremental.sql
12:03:28.182274 [debug] [MainThread]: Parsing macros\materializations\models\incremental\is_incremental.sql
12:03:28.185275 [debug] [MainThread]: Parsing macros\materializations\models\incremental\merge.sql
12:03:28.215293 [debug] [MainThread]: Parsing macros\materializations\models\incremental\on_schema_change.sql
12:03:28.261267 [debug] [MainThread]: Parsing macros\materializations\models\table\create_table_as.sql
12:03:28.270269 [debug] [MainThread]: Parsing macros\materializations\models\table\table.sql
12:03:28.291271 [debug] [MainThread]: Parsing macros\materializations\models\view\create_or_replace_view.sql
12:03:28.298272 [debug] [MainThread]: Parsing macros\materializations\models\view\create_view_as.sql
12:03:28.305292 [debug] [MainThread]: Parsing macros\materializations\models\view\helpers.sql
12:03:28.310293 [debug] [MainThread]: Parsing macros\materializations\models\view\view.sql
12:03:28.326272 [debug] [MainThread]: Parsing macros\materializations\seeds\helpers.sql
12:03:28.359293 [debug] [MainThread]: Parsing macros\materializations\seeds\seed.sql
12:03:28.377272 [debug] [MainThread]: Parsing macros\materializations\snapshots\helpers.sql
12:03:28.407267 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot.sql
12:03:28.427267 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot_merge.sql
12:03:28.430269 [debug] [MainThread]: Parsing macros\materializations\snapshots\strategies.sql
12:03:28.481270 [debug] [MainThread]: Parsing macros\materializations\tests\helpers.sql
12:03:28.486268 [debug] [MainThread]: Parsing macros\materializations\tests\test.sql
12:03:28.517270 [debug] [MainThread]: Parsing macros\materializations\tests\where_subquery.sql
12:03:28.527270 [debug] [MainThread]: Parsing tests\generic\builtin.sql
12:03:30.322090 [debug] [MainThread]: 1699: static parser successfully parsed example\100kusd.sql
12:03:30.360083 [debug] [MainThread]: 1699: static parser successfully parsed example\my_first_dbt_model.sql
12:03:30.372083 [debug] [MainThread]: 1699: static parser successfully parsed example\my_second_dbt_model.sql
12:03:30.697085 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1ba2f920-86da-4718-848f-9573525ecf02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002439D0C09D0>]}
12:03:30.726086 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1ba2f920-86da-4718-848f-9573525ecf02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000243949358E0>]}
12:03:30.727095 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 218 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
12:03:30.732086 [info ] [MainThread]: 
12:03:30.737088 [debug] [MainThread]: Acquiring new databricks connection "master"
12:03:30.740089 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
12:03:30.741089 [debug] [ThreadPool]: Using databricks connection "list_schemas"
12:03:30.742087 [debug] [ThreadPool]: On list_schemas: GetSchemas(database=None, schema=None)
12:03:30.743088 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:08:57.856318 [debug] [ThreadPool]: SQL status: OK in 327.11 seconds
12:08:57.920311 [debug] [ThreadPool]: On list_schemas: Close
12:09:07.263695 [debug] [ThreadPool]: Acquiring new databricks connection "create__dbt"
12:09:07.266704 [debug] [ThreadPool]: Acquiring new databricks connection "create__dbt"
12:09:07.267710 [debug] [ThreadPool]: Creating schema "_ReferenceKey(database=None, schema='dbt', identifier=None)"
12:09:07.339688 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
12:09:07.339688 [debug] [ThreadPool]: Using databricks connection "create__dbt"
12:09:07.340689 [debug] [ThreadPool]: On create__dbt: /* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "connection_name": "create__dbt"} */
create schema if not exists dbt
  
12:09:07.340689 [debug] [ThreadPool]: Opening a new connection, currently in state closed
12:09:12.261229 [debug] [ThreadPool]: SQL status: OK in 4.92 seconds
12:09:12.266233 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
12:09:12.268233 [debug] [ThreadPool]: On create__dbt: ROLLBACK
12:09:12.269232 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
12:09:12.270232 [debug] [ThreadPool]: On create__dbt: Close
12:09:13.399151 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_dbt"
12:09:13.431149 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
12:09:13.432142 [debug] [ThreadPool]: Using databricks connection "list_None_dbt"
12:09:13.433147 [debug] [ThreadPool]: On list_None_dbt: /* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "connection_name": "list_None_dbt"} */
show table extended in dbt like '*'
  
12:09:13.434166 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:09:16.267833 [debug] [ThreadPool]: SQL status: OK in 2.83 seconds
12:09:16.284830 [debug] [ThreadPool]: On list_None_dbt: ROLLBACK
12:09:16.285832 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
12:09:16.286831 [debug] [ThreadPool]: On list_None_dbt: Close
12:09:17.318729 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
12:09:17.318729 [debug] [MainThread]: Spark adapter: NotImplemented: commit
12:09:17.319729 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
12:09:17.320728 [info ] [MainThread]: 
12:09:17.376730 [debug] [Thread-1  ]: Began running node model.dbt_fivetran_1.100kusd
12:09:17.376730 [info ] [Thread-1  ]: 1 of 1 START view model dbt.100kusd............................................. [RUN]
12:09:17.378727 [debug] [Thread-1  ]: Acquiring new databricks connection "model.dbt_fivetran_1.100kusd"
12:09:17.378727 [debug] [Thread-1  ]: Began compiling node model.dbt_fivetran_1.100kusd
12:09:17.378727 [debug] [Thread-1  ]: Compiling model.dbt_fivetran_1.100kusd
12:09:17.382726 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_fivetran_1.100kusd"
12:09:17.387730 [debug] [Thread-1  ]: finished collecting timing info
12:09:17.387730 [debug] [Thread-1  ]: Began executing node model.dbt_fivetran_1.100kusd
12:09:17.427729 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_fivetran_1.100kusd"
12:09:17.430726 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
12:09:17.431727 [debug] [Thread-1  ]: Using databricks connection "model.dbt_fivetran_1.100kusd"
12:09:17.431727 [debug] [Thread-1  ]: On model.dbt_fivetran_1.100kusd: /* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "node_id": "model.dbt_fivetran_1.100kusd"} */
create or replace view dbt.100kusd
  
  
  as
    select DATE_CREATED,PARENT_ID from 100K_USD limit 10;

12:09:17.431727 [debug] [Thread-1  ]: Opening a new connection, currently in state init
12:09:20.034944 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "node_id": "model.dbt_fivetran_1.100kusd"} */
create or replace view dbt.100kusd
  
  
  as
    select DATE_CREATED,PARENT_ID from 100K_USD limit 10;

12:09:20.036960 [debug] [Thread-1  ]: Databricks adapter: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: 100K_USD; line 6 pos 39;
'GlobalLimit 10
+- 'LocalLimit 10
   +- 'Project ['DATE_CREATED, 'PARENT_ID]
      +- 'UnresolvedRelation [100K_USD], [], false

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: 100K_USD; line 6 pos 39;
'GlobalLimit 10
+- 'LocalLimit 10
   +- 'Project ['DATE_CREATED, 'PARENT_ID]
      +- 'UnresolvedRelation [100K_USD], [], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
	... 16 more

12:09:20.038923 [debug] [Thread-1  ]: finished collecting timing info
12:09:20.039922 [debug] [Thread-1  ]: On model.dbt_fivetran_1.100kusd: ROLLBACK
12:09:20.040923 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
12:09:20.040923 [debug] [Thread-1  ]: On model.dbt_fivetran_1.100kusd: Close
12:09:21.065923 [debug] [Thread-1  ]: Runtime Error in model 100kusd (models\example\100kusd.sql)
  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: 100K_USD; line 6 pos 39;
  'GlobalLimit 10
  +- 'LocalLimit 10
     +- 'Project ['DATE_CREATED, 'PARENT_ID]
        +- 'UnresolvedRelation [100K_USD], [], false
  
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
  	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
  	at java.security.AccessController.doPrivileged(Native Method)
  	at javax.security.auth.Subject.doAs(Subject.java:422)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
  	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	at java.lang.Thread.run(Thread.java:748)
  Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: 100K_USD; line 6 pos 39;
  'GlobalLimit 10
  +- 'LocalLimit 10
     +- 'Project ['DATE_CREATED, 'PARENT_ID]
        +- 'UnresolvedRelation [100K_USD], [], false
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
  	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
  	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
  	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
  	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
  	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
  	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
  	... 16 more
  
12:09:21.066925 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1ba2f920-86da-4718-848f-9573525ecf02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002439D3492E0>]}
12:09:21.066925 [error] [Thread-1  ]: 1 of 1 ERROR creating view model dbt.100kusd.................................... [[31mERROR[0m in 3.69s]
12:09:21.068920 [debug] [Thread-1  ]: Finished running node model.dbt_fivetran_1.100kusd
12:09:21.072923 [debug] [MainThread]: Acquiring new databricks connection "master"
12:09:21.073923 [debug] [MainThread]: On master: ROLLBACK
12:09:21.074919 [debug] [MainThread]: Opening a new connection, currently in state init
12:09:22.133380 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
12:09:22.134403 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
12:09:22.135394 [debug] [MainThread]: Spark adapter: NotImplemented: commit
12:09:22.136396 [debug] [MainThread]: On master: ROLLBACK
12:09:22.137394 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
12:09:22.138394 [debug] [MainThread]: On master: Close
12:09:23.217092 [info ] [MainThread]: 
12:09:23.220096 [info ] [MainThread]: Finished running 1 view model in 352.48s.
12:09:23.221099 [debug] [MainThread]: Connection 'master' was properly closed.
12:09:23.222098 [debug] [MainThread]: Connection 'create__dbt' was properly closed.
12:09:23.222098 [debug] [MainThread]: Connection 'list_None_dbt' was properly closed.
12:09:23.223092 [debug] [MainThread]: Connection 'model.dbt_fivetran_1.100kusd' was properly closed.
12:09:23.259095 [info ] [MainThread]: 
12:09:23.263122 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
12:09:23.278124 [info ] [MainThread]: 
12:09:23.280096 [error] [MainThread]: [33mRuntime Error in model 100kusd (models\example\100kusd.sql)[0m
12:09:23.286094 [error] [MainThread]:   org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: 100K_USD; line 6 pos 39;
12:09:23.292095 [error] [MainThread]:   'GlobalLimit 10
12:09:23.294094 [error] [MainThread]:   +- 'LocalLimit 10
12:09:23.301092 [error] [MainThread]:      +- 'Project ['DATE_CREATED, 'PARENT_ID]
12:09:23.306093 [error] [MainThread]:         +- 'UnresolvedRelation [100K_USD], [], false
12:09:23.314101 [error] [MainThread]:   
12:09:23.316090 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
12:09:23.320094 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
12:09:23.323094 [error] [MainThread]:   	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
12:09:23.324093 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
12:09:23.329099 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
12:09:23.333093 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
12:09:23.336096 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
12:09:23.339093 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
12:09:23.341093 [error] [MainThread]:   	at java.security.AccessController.doPrivileged(Native Method)
12:09:23.342092 [error] [MainThread]:   	at javax.security.auth.Subject.doAs(Subject.java:422)
12:09:23.344092 [error] [MainThread]:   	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
12:09:23.362096 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
12:09:23.364094 [error] [MainThread]:   	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
12:09:23.366095 [error] [MainThread]:   	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
12:09:23.369094 [error] [MainThread]:   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
12:09:23.370092 [error] [MainThread]:   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
12:09:23.373094 [error] [MainThread]:   	at java.lang.Thread.run(Thread.java:748)
12:09:23.376096 [error] [MainThread]:   Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: 100K_USD; line 6 pos 39;
12:09:23.384095 [error] [MainThread]:   'GlobalLimit 10
12:09:23.387110 [error] [MainThread]:   +- 'LocalLimit 10
12:09:23.397092 [error] [MainThread]:      +- 'Project ['DATE_CREATED, 'PARENT_ID]
12:09:23.398098 [error] [MainThread]:         +- 'UnresolvedRelation [100K_USD], [], false
12:09:23.401102 [error] [MainThread]:   
12:09:23.402096 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
12:09:23.403096 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
12:09:23.405100 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
12:09:23.409116 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
12:09:23.412096 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
12:09:23.417094 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
12:09:23.418094 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
12:09:23.420091 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
12:09:23.421092 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
12:09:23.423093 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
12:09:23.424095 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
12:09:23.426091 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
12:09:23.427092 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
12:09:23.430095 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
12:09:23.431095 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
12:09:23.433092 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
12:09:23.434091 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
12:09:23.435092 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
12:09:23.437096 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
12:09:23.439092 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
12:09:23.440095 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
12:09:23.444095 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
12:09:23.447095 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
12:09:23.451092 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
12:09:23.455095 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
12:09:23.464093 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
12:09:23.476098 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
12:09:23.482093 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
12:09:23.486098 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
12:09:23.489093 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
12:09:23.492092 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
12:09:23.494093 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
12:09:23.496095 [error] [MainThread]:   	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
12:09:23.497101 [error] [MainThread]:   	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
12:09:23.501095 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
12:09:23.503096 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
12:09:23.505094 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
12:09:23.506097 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
12:09:23.510094 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
12:09:23.512097 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
12:09:23.515093 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
12:09:23.516097 [error] [MainThread]:   	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
12:09:23.517094 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
12:09:23.520092 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
12:09:23.525096 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
12:09:23.527094 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
12:09:23.531102 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
12:09:23.533090 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
12:09:23.535093 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
12:09:23.537091 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
12:09:23.539094 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
12:09:23.542095 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
12:09:23.544102 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
12:09:23.546093 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
12:09:23.547096 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
12:09:23.548092 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
12:09:23.550093 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
12:09:23.552094 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
12:09:23.558110 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
12:09:23.560094 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
12:09:23.563093 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
12:09:23.565092 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
12:09:23.566094 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
12:09:23.567093 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
12:09:23.569092 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
12:09:23.571091 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
12:09:23.572093 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
12:09:23.577110 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
12:09:23.583094 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
12:09:23.586094 [error] [MainThread]:   	... 16 more
12:09:23.587096 [error] [MainThread]:   
12:09:23.588094 [info ] [MainThread]: 
12:09:23.591091 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
12:09:23.593092 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002439D0C0D90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002439CFCBDC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002439CFBEF10>]}


============================== 2022-06-17 15:56:20.727808 | 7aeb2fb9-5bd6-42a3-9e17-c2ee0dfe2c78 ==============================
15:56:20.727808 [info ] [MainThread]: Running with dbt=1.0.5
15:56:20.729133 [debug] [MainThread]: running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=None, defer=None, event_buffer_size=None, exclude=None, fail_fast=None, full_refresh=False, log_cache_events=False, log_format=None, partial_parse=None, printer_width=None, profile=None, profiles_dir='C:\\Users\\DELL\\.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=['example.100kusd'], selector_name=None, send_anonymous_usage_stats=None, single_threaded=False, state=None, static_parser=None, target=None, threads=None, use_colors=None, use_experimental_parser=None, vars='{}', version_check=None, warn_error=None, which='run', write_json=None)
15:56:20.730151 [debug] [MainThread]: Tracking: tracking
15:56:20.732251 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017D48A13CA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017D48A13D30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017D48A13D60>]}
15:56:20.888095 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
15:56:20.888095 [debug] [MainThread]: Partial parsing: updated file: dbt_fivetran_1://models\example\100kusd.sql
15:56:20.905116 [debug] [MainThread]: 1699: static parser successfully parsed example\100kusd.sql
15:56:20.936590 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7aeb2fb9-5bd6-42a3-9e17-c2ee0dfe2c78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017D48ADA6D0>]}
15:56:20.948574 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7aeb2fb9-5bd6-42a3-9e17-c2ee0dfe2c78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017D48A59CD0>]}
15:56:20.948574 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 218 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
15:56:20.951575 [info ] [MainThread]: 
15:56:20.952591 [debug] [MainThread]: Acquiring new databricks connection "master"
15:56:20.955567 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
15:56:20.955567 [debug] [ThreadPool]: Using databricks connection "list_schemas"
15:56:20.956567 [debug] [ThreadPool]: On list_schemas: GetSchemas(database=None, schema=None)
15:56:20.956567 [debug] [ThreadPool]: Opening a new connection, currently in state init
15:56:25.667540 [debug] [ThreadPool]: SQL status: OK in 4.71 seconds
15:56:25.709883 [debug] [ThreadPool]: On list_schemas: Close
15:56:35.041472 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_dbt"
15:56:35.084468 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
15:56:35.085470 [debug] [ThreadPool]: Using databricks connection "list_None_dbt"
15:56:35.085470 [debug] [ThreadPool]: On list_None_dbt: /* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "connection_name": "list_None_dbt"} */
show table extended in dbt like '*'
  
15:56:35.086478 [debug] [ThreadPool]: Opening a new connection, currently in state init
15:56:39.574700 [debug] [ThreadPool]: SQL status: OK in 4.49 seconds
15:56:39.587131 [debug] [ThreadPool]: On list_None_dbt: ROLLBACK
15:56:39.588135 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
15:56:39.589539 [debug] [ThreadPool]: On list_None_dbt: Close
15:56:40.707325 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
15:56:40.708748 [debug] [MainThread]: Spark adapter: NotImplemented: commit
15:56:40.709787 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
15:56:40.712324 [info ] [MainThread]: 
15:56:40.743372 [debug] [Thread-1  ]: Began running node model.dbt_fivetran_1.100kusd
15:56:40.745372 [info ] [Thread-1  ]: 1 of 1 START view model dbt.100kusd............................................. [RUN]
15:56:40.749362 [debug] [Thread-1  ]: Acquiring new databricks connection "model.dbt_fivetran_1.100kusd"
15:56:40.749362 [debug] [Thread-1  ]: Began compiling node model.dbt_fivetran_1.100kusd
15:56:40.750363 [debug] [Thread-1  ]: Compiling model.dbt_fivetran_1.100kusd
15:56:40.754359 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_fivetran_1.100kusd"
15:56:40.755366 [debug] [Thread-1  ]: finished collecting timing info
15:56:40.756358 [debug] [Thread-1  ]: Began executing node model.dbt_fivetran_1.100kusd
15:56:40.811358 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_fivetran_1.100kusd"
15:56:40.813360 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
15:56:40.813360 [debug] [Thread-1  ]: Using databricks connection "model.dbt_fivetran_1.100kusd"
15:56:40.814363 [debug] [Thread-1  ]: On model.dbt_fivetran_1.100kusd: /* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "node_id": "model.dbt_fivetran_1.100kusd"} */
create or replace view dbt.100kusd
  
  
  as
    select date_created,parent_id from 100k_usd limit 10;

15:56:40.814363 [debug] [Thread-1  ]: Opening a new connection, currently in state init
15:56:43.503847 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "node_id": "model.dbt_fivetran_1.100kusd"} */
create or replace view dbt.100kusd
  
  
  as
    select date_created,parent_id from 100k_usd limit 10;

15:56:43.504847 [debug] [Thread-1  ]: Databricks adapter: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
'GlobalLimit 10
+- 'LocalLimit 10
   +- 'Project ['date_created, 'parent_id]
      +- 'UnresolvedRelation [100k_usd], [], false

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
'GlobalLimit 10
+- 'LocalLimit 10
   +- 'Project ['date_created, 'parent_id]
      +- 'UnresolvedRelation [100k_usd], [], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
	... 16 more

15:56:43.505846 [debug] [Thread-1  ]: finished collecting timing info
15:56:43.506846 [debug] [Thread-1  ]: On model.dbt_fivetran_1.100kusd: ROLLBACK
15:56:43.507846 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
15:56:43.507846 [debug] [Thread-1  ]: On model.dbt_fivetran_1.100kusd: Close
15:56:44.564755 [debug] [Thread-1  ]: Runtime Error in model 100kusd (models\example\100kusd.sql)
  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
  'GlobalLimit 10
  +- 'LocalLimit 10
     +- 'Project ['date_created, 'parent_id]
        +- 'UnresolvedRelation [100k_usd], [], false
  
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
  	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
  	at java.security.AccessController.doPrivileged(Native Method)
  	at javax.security.auth.Subject.doAs(Subject.java:422)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
  	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	at java.lang.Thread.run(Thread.java:748)
  Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
  'GlobalLimit 10
  +- 'LocalLimit 10
     +- 'Project ['date_created, 'parent_id]
        +- 'UnresolvedRelation [100k_usd], [], false
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
  	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
  	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
  	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
  	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
  	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
  	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
  	... 16 more
  
15:56:44.565755 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7aeb2fb9-5bd6-42a3-9e17-c2ee0dfe2c78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017D48C06DF0>]}
15:56:44.567798 [error] [Thread-1  ]: 1 of 1 ERROR creating view model dbt.100kusd.................................... [[31mERROR[0m in 3.82s]
15:56:44.572171 [debug] [Thread-1  ]: Finished running node model.dbt_fivetran_1.100kusd
15:56:44.578213 [debug] [MainThread]: Acquiring new databricks connection "master"
15:56:44.580211 [debug] [MainThread]: On master: ROLLBACK
15:56:44.581211 [debug] [MainThread]: Opening a new connection, currently in state init
15:56:45.699164 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
15:56:45.700164 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
15:56:45.701165 [debug] [MainThread]: Spark adapter: NotImplemented: commit
15:56:45.702164 [debug] [MainThread]: On master: ROLLBACK
15:56:45.702164 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
15:56:45.703164 [debug] [MainThread]: On master: Close
15:56:46.792153 [info ] [MainThread]: 
15:56:46.794537 [info ] [MainThread]: Finished running 1 view model in 25.84s.
15:56:46.796563 [debug] [MainThread]: Connection 'master' was properly closed.
15:56:46.797572 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
15:56:46.797572 [debug] [MainThread]: Connection 'list_None_dbt' was properly closed.
15:56:46.798567 [debug] [MainThread]: Connection 'model.dbt_fivetran_1.100kusd' was properly closed.
15:56:46.814729 [info ] [MainThread]: 
15:56:46.815728 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
15:56:46.817730 [info ] [MainThread]: 
15:56:46.818735 [error] [MainThread]: [33mRuntime Error in model 100kusd (models\example\100kusd.sql)[0m
15:56:46.819728 [error] [MainThread]:   org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
15:56:46.820729 [error] [MainThread]:   'GlobalLimit 10
15:56:46.821730 [error] [MainThread]:   +- 'LocalLimit 10
15:56:46.822732 [error] [MainThread]:      +- 'Project ['date_created, 'parent_id]
15:56:46.824729 [error] [MainThread]:         +- 'UnresolvedRelation [100k_usd], [], false
15:56:46.825730 [error] [MainThread]:   
15:56:46.826729 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
15:56:46.827729 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
15:56:46.829729 [error] [MainThread]:   	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
15:56:46.830733 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
15:56:46.832730 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
15:56:46.834732 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
15:56:46.836736 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
15:56:46.838743 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
15:56:46.840729 [error] [MainThread]:   	at java.security.AccessController.doPrivileged(Native Method)
15:56:46.841730 [error] [MainThread]:   	at javax.security.auth.Subject.doAs(Subject.java:422)
15:56:46.843732 [error] [MainThread]:   	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
15:56:46.844732 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
15:56:46.846734 [error] [MainThread]:   	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
15:56:46.847731 [error] [MainThread]:   	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
15:56:46.848731 [error] [MainThread]:   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
15:56:46.850730 [error] [MainThread]:   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
15:56:46.853740 [error] [MainThread]:   	at java.lang.Thread.run(Thread.java:748)
15:56:46.854733 [error] [MainThread]:   Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
15:56:46.856734 [error] [MainThread]:   'GlobalLimit 10
15:56:46.857731 [error] [MainThread]:   +- 'LocalLimit 10
15:56:46.859731 [error] [MainThread]:      +- 'Project ['date_created, 'parent_id]
15:56:46.860730 [error] [MainThread]:         +- 'UnresolvedRelation [100k_usd], [], false
15:56:46.861729 [error] [MainThread]:   
15:56:46.863746 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
15:56:46.864730 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
15:56:46.867734 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
15:56:46.869737 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
15:56:46.871730 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
15:56:46.872729 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
15:56:46.875733 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
15:56:46.876732 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
15:56:46.877730 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
15:56:46.879735 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
15:56:46.881728 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
15:56:46.882729 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
15:56:46.883731 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
15:56:46.884737 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
15:56:46.885731 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
15:56:46.886730 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
15:56:46.888732 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
15:56:46.889732 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
15:56:46.890733 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
15:56:46.891731 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
15:56:46.894730 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
15:56:46.896731 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
15:56:46.897728 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
15:56:46.898728 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
15:56:46.899732 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
15:56:46.900729 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
15:56:46.905736 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
15:56:46.907734 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
15:56:46.908732 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
15:56:46.909731 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
15:56:46.910731 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
15:56:46.912731 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
15:56:46.914729 [error] [MainThread]:   	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
15:56:46.916729 [error] [MainThread]:   	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
15:56:46.917732 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
15:56:46.919729 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
15:56:46.920732 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
15:56:46.922730 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
15:56:46.923732 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
15:56:46.924730 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
15:56:46.928733 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
15:56:46.932746 [error] [MainThread]:   	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
15:56:46.934731 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
15:56:46.937731 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
15:56:46.939729 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
15:56:46.940732 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
15:56:46.941731 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
15:56:46.943732 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
15:56:46.944729 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
15:56:46.945730 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
15:56:46.947729 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
15:56:46.948729 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
15:56:46.950728 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
15:56:46.951730 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
15:56:46.952729 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
15:56:46.953730 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
15:56:46.956731 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
15:56:46.957730 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
15:56:46.959731 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
15:56:46.962741 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
15:56:46.963731 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
15:56:46.965730 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
15:56:46.966730 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
15:56:46.968735 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
15:56:46.970728 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
15:56:46.972728 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
15:56:46.973731 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
15:56:46.976734 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
15:56:46.977731 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
15:56:46.981740 [error] [MainThread]:   	... 16 more
15:56:46.982730 [error] [MainThread]:   
15:56:46.984727 [info ] [MainThread]: 
15:56:46.985729 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
15:56:46.986730 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017D489B2FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017D48C14940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017D48C14070>]}


============================== 2022-06-17 16:04:55.783386 | cc4c44e3-73ea-44e4-8b9c-828587639aaf ==============================
16:04:55.783386 [info ] [MainThread]: Running with dbt=1.0.5
16:04:55.784387 [debug] [MainThread]: running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=None, defer=None, event_buffer_size=None, exclude=None, fail_fast=None, full_refresh=False, log_cache_events=False, log_format=None, partial_parse=None, printer_width=None, profile=None, profiles_dir='C:\\Users\\DELL\\.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=['example.100kusd'], selector_name=None, send_anonymous_usage_stats=None, single_threaded=False, state=None, static_parser=None, target=None, threads=None, use_colors=None, use_experimental_parser=None, vars='{}', version_check=None, warn_error=None, which='run', write_json=None)
16:04:55.785369 [debug] [MainThread]: Tracking: tracking
16:04:55.786389 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E19F3A2CA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E19F3A2D30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E19F3A2D60>]}
16:04:55.901505 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
16:04:55.902506 [debug] [MainThread]: Partial parsing: update schema file: dbt_fivetran_1://models\example\schema.yml
16:04:55.959659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cc4c44e3-73ea-44e4-8b9c-828587639aaf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E19F4B4FA0>]}
16:04:55.968679 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cc4c44e3-73ea-44e4-8b9c-828587639aaf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E19F413D00>]}
16:04:55.969684 [info ] [MainThread]: Found 3 models, 6 tests, 0 snapshots, 0 analyses, 218 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
16:04:55.971686 [info ] [MainThread]: 
16:04:55.972680 [debug] [MainThread]: Acquiring new databricks connection "master"
16:04:55.974660 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
16:04:55.975665 [debug] [ThreadPool]: Using databricks connection "list_schemas"
16:04:55.975665 [debug] [ThreadPool]: On list_schemas: GetSchemas(database=None, schema=None)
16:04:55.976663 [debug] [ThreadPool]: Opening a new connection, currently in state init
16:04:58.327045 [debug] [ThreadPool]: SQL status: OK in 2.35 seconds
16:04:58.335780 [debug] [ThreadPool]: On list_schemas: Close
16:04:59.376769 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_dbt"
16:04:59.417650 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
16:04:59.417992 [debug] [ThreadPool]: Using databricks connection "list_None_dbt"
16:04:59.419036 [debug] [ThreadPool]: On list_None_dbt: /* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "connection_name": "list_None_dbt"} */
show table extended in dbt like '*'
  
16:04:59.420036 [debug] [ThreadPool]: Opening a new connection, currently in state init
16:05:02.146367 [debug] [ThreadPool]: SQL status: OK in 2.73 seconds
16:05:02.152369 [debug] [ThreadPool]: On list_None_dbt: ROLLBACK
16:05:02.153368 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
16:05:02.153368 [debug] [ThreadPool]: On list_None_dbt: Close
16:05:03.261495 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
16:05:03.262491 [debug] [MainThread]: Spark adapter: NotImplemented: commit
16:05:03.265489 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
16:05:03.266488 [info ] [MainThread]: 
16:05:03.284484 [debug] [Thread-1  ]: Began running node model.dbt_fivetran_1.100kusd
16:05:03.285485 [info ] [Thread-1  ]: 1 of 1 START view model dbt.100kusd............................................. [RUN]
16:05:03.287483 [debug] [Thread-1  ]: Acquiring new databricks connection "model.dbt_fivetran_1.100kusd"
16:05:03.287483 [debug] [Thread-1  ]: Began compiling node model.dbt_fivetran_1.100kusd
16:05:03.288493 [debug] [Thread-1  ]: Compiling model.dbt_fivetran_1.100kusd
16:05:03.295493 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_fivetran_1.100kusd"
16:05:03.299488 [debug] [Thread-1  ]: finished collecting timing info
16:05:03.300484 [debug] [Thread-1  ]: Began executing node model.dbt_fivetran_1.100kusd
16:05:03.362482 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_fivetran_1.100kusd"
16:05:03.363480 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
16:05:03.363480 [debug] [Thread-1  ]: Using databricks connection "model.dbt_fivetran_1.100kusd"
16:05:03.364481 [debug] [Thread-1  ]: On model.dbt_fivetran_1.100kusd: /* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "node_id": "model.dbt_fivetran_1.100kusd"} */
create or replace view dbt.100kusd
  
  
  as
    select date_created,parent_id from 100k_usd limit 10;

16:05:03.364481 [debug] [Thread-1  ]: Opening a new connection, currently in state init
16:05:05.738212 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "node_id": "model.dbt_fivetran_1.100kusd"} */
create or replace view dbt.100kusd
  
  
  as
    select date_created,parent_id from 100k_usd limit 10;

16:05:05.739255 [debug] [Thread-1  ]: Databricks adapter: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
'GlobalLimit 10
+- 'LocalLimit 10
   +- 'Project ['date_created, 'parent_id]
      +- 'UnresolvedRelation [100k_usd], [], false

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
'GlobalLimit 10
+- 'LocalLimit 10
   +- 'Project ['date_created, 'parent_id]
      +- 'UnresolvedRelation [100k_usd], [], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
	... 16 more

16:05:05.741255 [debug] [Thread-1  ]: finished collecting timing info
16:05:05.742205 [debug] [Thread-1  ]: On model.dbt_fivetran_1.100kusd: ROLLBACK
16:05:05.742205 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
16:05:05.743255 [debug] [Thread-1  ]: On model.dbt_fivetran_1.100kusd: Close
16:05:06.848367 [debug] [Thread-1  ]: Runtime Error in model 100kusd (models\example\100kusd.sql)
  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
  'GlobalLimit 10
  +- 'LocalLimit 10
     +- 'Project ['date_created, 'parent_id]
        +- 'UnresolvedRelation [100k_usd], [], false
  
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
  	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
  	at java.security.AccessController.doPrivileged(Native Method)
  	at javax.security.auth.Subject.doAs(Subject.java:422)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
  	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	at java.lang.Thread.run(Thread.java:748)
  Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
  'GlobalLimit 10
  +- 'LocalLimit 10
     +- 'Project ['date_created, 'parent_id]
        +- 'UnresolvedRelation [100k_usd], [], false
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
  	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
  	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
  	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
  	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
  	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
  	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
  	... 16 more
  
16:05:06.850381 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cc4c44e3-73ea-44e4-8b9c-828587639aaf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E19F5B0B50>]}
16:05:06.851379 [error] [Thread-1  ]: 1 of 1 ERROR creating view model dbt.100kusd.................................... [[31mERROR[0m in 3.56s]
16:05:06.855382 [debug] [Thread-1  ]: Finished running node model.dbt_fivetran_1.100kusd
16:05:06.861362 [debug] [MainThread]: Acquiring new databricks connection "master"
16:05:06.862360 [debug] [MainThread]: On master: ROLLBACK
16:05:06.863364 [debug] [MainThread]: Opening a new connection, currently in state init
16:05:07.943057 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
16:05:07.944055 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
16:05:07.945083 [debug] [MainThread]: Spark adapter: NotImplemented: commit
16:05:07.945083 [debug] [MainThread]: On master: ROLLBACK
16:05:07.946084 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
16:05:07.947083 [debug] [MainThread]: On master: Close
16:05:09.032816 [info ] [MainThread]: 
16:05:09.035819 [info ] [MainThread]: Finished running 1 view model in 13.06s.
16:05:09.038837 [debug] [MainThread]: Connection 'master' was properly closed.
16:05:09.040824 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
16:05:09.041825 [debug] [MainThread]: Connection 'list_None_dbt' was properly closed.
16:05:09.042819 [debug] [MainThread]: Connection 'model.dbt_fivetran_1.100kusd' was properly closed.
16:05:09.070672 [info ] [MainThread]: 
16:05:09.071668 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
16:05:09.073669 [info ] [MainThread]: 
16:05:09.077685 [error] [MainThread]: [33mRuntime Error in model 100kusd (models\example\100kusd.sql)[0m
16:05:09.080686 [error] [MainThread]:   org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
16:05:09.082674 [error] [MainThread]:   'GlobalLimit 10
16:05:09.083671 [error] [MainThread]:   +- 'LocalLimit 10
16:05:09.085675 [error] [MainThread]:      +- 'Project ['date_created, 'parent_id]
16:05:09.086670 [error] [MainThread]:         +- 'UnresolvedRelation [100k_usd], [], false
16:05:09.087671 [error] [MainThread]:   
16:05:09.089673 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
16:05:09.094671 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
16:05:09.096670 [error] [MainThread]:   	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
16:05:09.098671 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
16:05:09.099673 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
16:05:09.101670 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
16:05:09.103670 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
16:05:09.104671 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
16:05:09.107671 [error] [MainThread]:   	at java.security.AccessController.doPrivileged(Native Method)
16:05:09.109676 [error] [MainThread]:   	at javax.security.auth.Subject.doAs(Subject.java:422)
16:05:09.110671 [error] [MainThread]:   	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
16:05:09.111670 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
16:05:09.113669 [error] [MainThread]:   	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
16:05:09.114671 [error] [MainThread]:   	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
16:05:09.119675 [error] [MainThread]:   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
16:05:09.121679 [error] [MainThread]:   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
16:05:09.123670 [error] [MainThread]:   	at java.lang.Thread.run(Thread.java:748)
16:05:09.125705 [error] [MainThread]:   Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
16:05:09.127675 [error] [MainThread]:   'GlobalLimit 10
16:05:09.129672 [error] [MainThread]:   +- 'LocalLimit 10
16:05:09.130672 [error] [MainThread]:      +- 'Project ['date_created, 'parent_id]
16:05:09.131672 [error] [MainThread]:         +- 'UnresolvedRelation [100k_usd], [], false
16:05:09.132672 [error] [MainThread]:   
16:05:09.134672 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
16:05:09.137678 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
16:05:09.138670 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
16:05:09.140672 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
16:05:09.142672 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
16:05:09.143670 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
16:05:09.144671 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
16:05:09.145672 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
16:05:09.146671 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
16:05:09.148670 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
16:05:09.149671 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
16:05:09.150675 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
16:05:09.151671 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
16:05:09.152672 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
16:05:09.154670 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
16:05:09.155668 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
16:05:09.156670 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
16:05:09.158674 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
16:05:09.160673 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
16:05:09.161674 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
16:05:09.162669 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
16:05:09.163668 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
16:05:09.164668 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
16:05:09.166671 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
16:05:09.170671 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
16:05:09.172678 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
16:05:09.174673 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
16:05:09.175670 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
16:05:09.176673 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
16:05:09.177670 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
16:05:09.178673 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
16:05:09.179671 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
16:05:09.180671 [error] [MainThread]:   	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
16:05:09.181671 [error] [MainThread]:   	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
16:05:09.183676 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
16:05:09.184673 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
16:05:09.185674 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
16:05:09.186670 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
16:05:09.187669 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
16:05:09.189672 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
16:05:09.191671 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
16:05:09.193673 [error] [MainThread]:   	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
16:05:09.196673 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
16:05:09.199677 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
16:05:09.200670 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
16:05:09.201670 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
16:05:09.203672 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
16:05:09.204670 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
16:05:09.205674 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
16:05:09.206671 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
16:05:09.208693 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
16:05:09.210671 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
16:05:09.212675 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
16:05:09.214671 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
16:05:09.216668 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
16:05:09.217670 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
16:05:09.218674 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
16:05:09.219672 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
16:05:09.220669 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
16:05:09.221671 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
16:05:09.222671 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
16:05:09.224670 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
16:05:09.225676 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
16:05:09.226669 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
16:05:09.227671 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
16:05:09.229667 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
16:05:09.230668 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
16:05:09.232668 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
16:05:09.233670 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
16:05:09.234671 [error] [MainThread]:   	... 16 more
16:05:09.235669 [error] [MainThread]:   
16:05:09.236670 [info ] [MainThread]: 
16:05:09.237668 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
16:05:09.238668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E19F3CFD90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E19F3CF9A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E19F598700>]}


============================== 2022-06-21 14:58:33.133204 | 8ca2e17f-7105-4725-b6d3-43bf60846b55 ==============================
14:58:33.133204 [info ] [MainThread]: Running with dbt=1.0.5
14:58:33.134205 [debug] [MainThread]: running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=None, defer=None, event_buffer_size=None, exclude=None, fail_fast=None, full_refresh=False, log_cache_events=False, log_format=None, partial_parse=None, printer_width=None, profile=None, profiles_dir='C:\\Users\\DELL\\.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=['example.100kusd'], selector_name=None, send_anonymous_usage_stats=None, single_threaded=False, state=None, static_parser=None, target=None, threads=None, use_colors=None, use_experimental_parser=None, vars='{}', version_check=None, warn_error=None, which='run', write_json=None)
14:58:33.135207 [debug] [MainThread]: Tracking: tracking
14:58:33.138206 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000263E8F33D60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000263E8F33DF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000263E8F33E20>]}
14:58:33.387207 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
14:58:33.387207 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
14:58:33.408208 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8ca2e17f-7105-4725-b6d3-43bf60846b55', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000263E9003BB0>]}
14:58:33.424208 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8ca2e17f-7105-4725-b6d3-43bf60846b55', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000263E8FEBE50>]}
14:58:33.425205 [info ] [MainThread]: Found 3 models, 6 tests, 0 snapshots, 0 analyses, 218 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
14:58:33.429210 [info ] [MainThread]: 
14:58:33.431205 [debug] [MainThread]: Acquiring new databricks connection "master"
14:58:33.435209 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
14:58:33.436208 [debug] [ThreadPool]: Using databricks connection "list_schemas"
14:58:33.436208 [debug] [ThreadPool]: On list_schemas: GetSchemas(database=None, schema=None)
14:58:33.437208 [debug] [ThreadPool]: Opening a new connection, currently in state init
14:58:36.885353 [debug] [ThreadPool]: SQL status: OK in 3.45 seconds
14:58:36.934364 [debug] [ThreadPool]: On list_schemas: Close
14:58:48.608538 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_dbt"
14:58:48.656555 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
14:58:48.657554 [debug] [ThreadPool]: Using databricks connection "list_None_dbt"
14:58:48.658557 [debug] [ThreadPool]: On list_None_dbt: /* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "connection_name": "list_None_dbt"} */
show table extended in dbt like '*'
  
14:58:48.658557 [debug] [ThreadPool]: Opening a new connection, currently in state init
14:58:51.720535 [debug] [ThreadPool]: SQL status: OK in 3.06 seconds
14:58:51.734325 [debug] [ThreadPool]: On list_None_dbt: ROLLBACK
14:58:51.734325 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
14:58:51.736327 [debug] [ThreadPool]: On list_None_dbt: Close
14:58:53.027237 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
14:58:53.028244 [debug] [MainThread]: Spark adapter: NotImplemented: commit
14:58:53.029243 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
14:58:53.031245 [info ] [MainThread]: 
14:58:53.052238 [debug] [Thread-1  ]: Began running node model.dbt_fivetran_1.100kusd
14:58:53.054242 [info ] [Thread-1  ]: 1 of 1 START view model dbt.100kusd............................................. [RUN]
14:58:53.057234 [debug] [Thread-1  ]: Acquiring new databricks connection "model.dbt_fivetran_1.100kusd"
14:58:53.058236 [debug] [Thread-1  ]: Began compiling node model.dbt_fivetran_1.100kusd
14:58:53.058236 [debug] [Thread-1  ]: Compiling model.dbt_fivetran_1.100kusd
14:58:53.063239 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_fivetran_1.100kusd"
14:58:53.065237 [debug] [Thread-1  ]: finished collecting timing info
14:58:53.066237 [debug] [Thread-1  ]: Began executing node model.dbt_fivetran_1.100kusd
14:58:53.156235 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_fivetran_1.100kusd"
14:58:53.159235 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
14:58:53.159235 [debug] [Thread-1  ]: Using databricks connection "model.dbt_fivetran_1.100kusd"
14:58:53.159235 [debug] [Thread-1  ]: On model.dbt_fivetran_1.100kusd: /* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "node_id": "model.dbt_fivetran_1.100kusd"} */
create or replace view dbt.100kusd
  
  
  as
    select date_created,parent_id from 100k_usd limit 10;

14:58:53.160233 [debug] [Thread-1  ]: Opening a new connection, currently in state init
14:58:55.602211 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "node_id": "model.dbt_fivetran_1.100kusd"} */
create or replace view dbt.100kusd
  
  
  as
    select date_created,parent_id from 100k_usd limit 10;

14:58:55.603259 [debug] [Thread-1  ]: Databricks adapter: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
'GlobalLimit 10
+- 'LocalLimit 10
   +- 'Project ['date_created, 'parent_id]
      +- 'UnresolvedRelation [100k_usd], [], false

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
'GlobalLimit 10
+- 'LocalLimit 10
   +- 'Project ['date_created, 'parent_id]
      +- 'UnresolvedRelation [100k_usd], [], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
	... 16 more

14:58:55.604255 [debug] [Thread-1  ]: finished collecting timing info
14:58:55.605254 [debug] [Thread-1  ]: On model.dbt_fivetran_1.100kusd: ROLLBACK
14:58:55.606254 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
14:58:55.606254 [debug] [Thread-1  ]: On model.dbt_fivetran_1.100kusd: Close
14:58:56.741740 [debug] [Thread-1  ]: Runtime Error in model 100kusd (models\example\100kusd.sql)
  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
  'GlobalLimit 10
  +- 'LocalLimit 10
     +- 'Project ['date_created, 'parent_id]
        +- 'UnresolvedRelation [100k_usd], [], false
  
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
  	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
  	at java.security.AccessController.doPrivileged(Native Method)
  	at javax.security.auth.Subject.doAs(Subject.java:422)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
  	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	at java.lang.Thread.run(Thread.java:748)
  Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
  'GlobalLimit 10
  +- 'LocalLimit 10
     +- 'Project ['date_created, 'parent_id]
        +- 'UnresolvedRelation [100k_usd], [], false
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
  	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
  	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
  	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
  	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
  	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
  	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
  	... 16 more
  
14:58:56.743739 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8ca2e17f-7105-4725-b6d3-43bf60846b55', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000263E9137160>]}
14:58:56.744739 [error] [Thread-1  ]: 1 of 1 ERROR creating view model dbt.100kusd.................................... [[31mERROR[0m in 3.69s]
14:58:56.749700 [debug] [Thread-1  ]: Finished running node model.dbt_fivetran_1.100kusd
14:58:56.756715 [debug] [MainThread]: Acquiring new databricks connection "master"
14:58:56.757714 [debug] [MainThread]: On master: ROLLBACK
14:58:56.758717 [debug] [MainThread]: Opening a new connection, currently in state init
14:58:57.919386 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
14:58:57.920388 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
14:58:57.921388 [debug] [MainThread]: Spark adapter: NotImplemented: commit
14:58:57.921388 [debug] [MainThread]: On master: ROLLBACK
14:58:57.922388 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
14:58:57.923386 [debug] [MainThread]: On master: Close
14:58:58.955991 [info ] [MainThread]: 
14:58:58.957965 [info ] [MainThread]: Finished running 1 view model in 25.52s.
14:58:58.958968 [debug] [MainThread]: Connection 'master' was properly closed.
14:58:58.959963 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
14:58:58.959963 [debug] [MainThread]: Connection 'list_None_dbt' was properly closed.
14:58:58.960992 [debug] [MainThread]: Connection 'model.dbt_fivetran_1.100kusd' was properly closed.
14:58:58.984974 [info ] [MainThread]: 
14:58:58.986967 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
14:58:58.991978 [info ] [MainThread]: 
14:58:58.993966 [error] [MainThread]: [33mRuntime Error in model 100kusd (models\example\100kusd.sql)[0m
14:58:58.997962 [error] [MainThread]:   org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
14:58:58.999962 [error] [MainThread]:   'GlobalLimit 10
14:58:59.000962 [error] [MainThread]:   +- 'LocalLimit 10
14:58:59.002964 [error] [MainThread]:      +- 'Project ['date_created, 'parent_id]
14:58:59.003965 [error] [MainThread]:         +- 'UnresolvedRelation [100k_usd], [], false
14:58:59.005964 [error] [MainThread]:   
14:58:59.007964 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
14:58:59.012965 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
14:58:59.015967 [error] [MainThread]:   	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
14:58:59.016965 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
14:58:59.018963 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
14:58:59.020961 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
14:58:59.022962 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
14:58:59.023962 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
14:58:59.026963 [error] [MainThread]:   	at java.security.AccessController.doPrivileged(Native Method)
14:58:59.029963 [error] [MainThread]:   	at javax.security.auth.Subject.doAs(Subject.java:422)
14:58:59.031968 [error] [MainThread]:   	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
14:58:59.033967 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
14:58:59.034962 [error] [MainThread]:   	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
14:58:59.035968 [error] [MainThread]:   	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
14:58:59.037960 [error] [MainThread]:   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
14:58:59.038962 [error] [MainThread]:   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
14:58:59.040966 [error] [MainThread]:   	at java.lang.Thread.run(Thread.java:748)
14:58:59.046964 [error] [MainThread]:   Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
14:58:59.048964 [error] [MainThread]:   'GlobalLimit 10
14:58:59.049962 [error] [MainThread]:   +- 'LocalLimit 10
14:58:59.051961 [error] [MainThread]:      +- 'Project ['date_created, 'parent_id]
14:58:59.053962 [error] [MainThread]:         +- 'UnresolvedRelation [100k_usd], [], false
14:58:59.054962 [error] [MainThread]:   
14:58:59.057962 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
14:58:59.060963 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
14:58:59.062960 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
14:58:59.063961 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
14:58:59.065964 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
14:58:59.066959 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
14:58:59.067961 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
14:58:59.069962 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
14:58:59.070962 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
14:58:59.071960 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
14:58:59.072962 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
14:58:59.073962 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
14:58:59.074962 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
14:58:59.075963 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
14:58:59.076960 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
14:58:59.078965 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
14:58:59.079960 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
14:58:59.080961 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
14:58:59.081963 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
14:58:59.082962 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
14:58:59.083962 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
14:58:59.084962 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
14:58:59.085962 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
14:58:59.086959 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
14:58:59.088960 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
14:58:59.089960 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
14:58:59.090962 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
14:58:59.091961 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
14:58:59.092961 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
14:58:59.093963 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
14:58:59.094962 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
14:58:59.096960 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
14:58:59.097963 [error] [MainThread]:   	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
14:58:59.098964 [error] [MainThread]:   	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
14:58:59.100967 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
14:58:59.102960 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
14:58:59.104961 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
14:58:59.106960 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
14:58:59.107964 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
14:58:59.108963 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
14:58:59.109961 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
14:58:59.110963 [error] [MainThread]:   	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
14:58:59.112962 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
14:58:59.113963 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
14:58:59.115961 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
14:58:59.116962 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
14:58:59.118963 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
14:58:59.120964 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
14:58:59.121963 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
14:58:59.123961 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
14:58:59.124960 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
14:58:59.125961 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
14:58:59.126963 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
14:58:59.128962 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
14:58:59.129963 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
14:58:59.131962 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
14:58:59.132961 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
14:58:59.133963 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
14:58:59.135961 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
14:58:59.136961 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
14:58:59.137960 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
14:58:59.139963 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
14:58:59.142981 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
14:58:59.144965 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
14:58:59.146964 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
14:58:59.148963 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
14:58:59.151961 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
14:58:59.153962 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
14:58:59.154960 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
14:58:59.155960 [error] [MainThread]:   	... 16 more
14:58:59.156962 [error] [MainThread]:   
14:58:59.158959 [info ] [MainThread]: 
14:58:59.160963 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
14:58:59.162964 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000263E8F4B700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000263E8ED8B80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000263E8FF34C0>]}


============================== 2022-06-21 15:17:16.683092 | ba0eda5e-9d79-4ce6-aa1c-3f2f49e04bab ==============================
15:17:16.683092 [info ] [MainThread]: Running with dbt=1.0.5
15:17:16.684092 [debug] [MainThread]: running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=None, defer=None, event_buffer_size=None, exclude=None, fail_fast=None, full_refresh=False, log_cache_events=False, log_format=None, partial_parse=None, printer_width=None, profile=None, profiles_dir='C:\\Users\\DELL\\.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=['example.100kusd'], selector_name=None, send_anonymous_usage_stats=None, single_threaded=False, state=None, static_parser=None, target=None, threads=None, use_colors=None, use_experimental_parser=None, vars='{}', version_check=None, warn_error=None, which='run', write_json=None)
15:17:16.685088 [debug] [MainThread]: Tracking: tracking
15:17:16.686089 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AF47F2D60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AF47F2DF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AF47F2E20>]}
15:17:16.841085 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
15:17:16.841085 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
15:17:16.849089 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ba0eda5e-9d79-4ce6-aa1c-3f2f49e04bab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AF4902EB0>]}
15:17:16.860086 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ba0eda5e-9d79-4ce6-aa1c-3f2f49e04bab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AF48EAE50>]}
15:17:16.860086 [info ] [MainThread]: Found 3 models, 6 tests, 0 snapshots, 0 analyses, 218 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
15:17:16.863088 [info ] [MainThread]: 
15:17:16.864086 [debug] [MainThread]: Acquiring new databricks connection "master"
15:17:16.867089 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
15:17:16.867089 [debug] [ThreadPool]: Using databricks connection "list_schemas"
15:17:16.868089 [debug] [ThreadPool]: On list_schemas: GetSchemas(database=None, schema=None)
15:17:16.868089 [debug] [ThreadPool]: Opening a new connection, currently in state init
15:17:19.297871 [debug] [ThreadPool]: SQL status: OK in 2.43 seconds
15:17:19.313877 [debug] [ThreadPool]: On list_schemas: Close
15:17:20.609941 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_dbt"
15:17:20.650005 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
15:17:20.650005 [debug] [ThreadPool]: Using databricks connection "list_None_dbt"
15:17:20.651005 [debug] [ThreadPool]: On list_None_dbt: /* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "connection_name": "list_None_dbt"} */
show table extended in dbt like '*'
  
15:17:20.651005 [debug] [ThreadPool]: Opening a new connection, currently in state init
15:17:23.108676 [debug] [ThreadPool]: SQL status: OK in 2.46 seconds
15:17:23.120423 [debug] [ThreadPool]: On list_None_dbt: ROLLBACK
15:17:23.120423 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
15:17:23.122393 [debug] [ThreadPool]: On list_None_dbt: Close
15:17:24.210085 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
15:17:24.211084 [debug] [MainThread]: Spark adapter: NotImplemented: commit
15:17:24.212083 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
15:17:24.214223 [info ] [MainThread]: 
15:17:24.236267 [debug] [Thread-1  ]: Began running node model.dbt_fivetran_1.100kusd
15:17:24.239268 [info ] [Thread-1  ]: 1 of 1 START view model dbt.100kusd............................................. [RUN]
15:17:24.245259 [debug] [Thread-1  ]: Acquiring new databricks connection "model.dbt_fivetran_1.100kusd"
15:17:24.246261 [debug] [Thread-1  ]: Began compiling node model.dbt_fivetran_1.100kusd
15:17:24.246261 [debug] [Thread-1  ]: Compiling model.dbt_fivetran_1.100kusd
15:17:24.249257 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_fivetran_1.100kusd"
15:17:24.251257 [debug] [Thread-1  ]: finished collecting timing info
15:17:24.251257 [debug] [Thread-1  ]: Began executing node model.dbt_fivetran_1.100kusd
15:17:24.322261 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_fivetran_1.100kusd"
15:17:24.324258 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
15:17:24.324258 [debug] [Thread-1  ]: Using databricks connection "model.dbt_fivetran_1.100kusd"
15:17:24.325260 [debug] [Thread-1  ]: On model.dbt_fivetran_1.100kusd: /* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "node_id": "model.dbt_fivetran_1.100kusd"} */
create or replace view dbt.100kusd
  
  
  as
    select date_created,parent_id from 100k_usd limit 10;

15:17:24.326257 [debug] [Thread-1  ]: Opening a new connection, currently in state init
15:17:26.712197 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "node_id": "model.dbt_fivetran_1.100kusd"} */
create or replace view dbt.100kusd
  
  
  as
    select date_created,parent_id from 100k_usd limit 10;

15:17:26.713198 [debug] [Thread-1  ]: Databricks adapter: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
'GlobalLimit 10
+- 'LocalLimit 10
   +- 'Project ['date_created, 'parent_id]
      +- 'UnresolvedRelation [100k_usd], [], false

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
'GlobalLimit 10
+- 'LocalLimit 10
   +- 'Project ['date_created, 'parent_id]
      +- 'UnresolvedRelation [100k_usd], [], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
	... 16 more

15:17:26.714198 [debug] [Thread-1  ]: finished collecting timing info
15:17:26.715197 [debug] [Thread-1  ]: On model.dbt_fivetran_1.100kusd: ROLLBACK
15:17:26.716197 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
15:17:26.716197 [debug] [Thread-1  ]: On model.dbt_fivetran_1.100kusd: Close
15:17:27.898648 [debug] [Thread-1  ]: Runtime Error in model 100kusd (models\example\100kusd.sql)
  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
  'GlobalLimit 10
  +- 'LocalLimit 10
     +- 'Project ['date_created, 'parent_id]
        +- 'UnresolvedRelation [100k_usd], [], false
  
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
  	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
  	at java.security.AccessController.doPrivileged(Native Method)
  	at javax.security.auth.Subject.doAs(Subject.java:422)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
  	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	at java.lang.Thread.run(Thread.java:748)
  Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
  'GlobalLimit 10
  +- 'LocalLimit 10
     +- 'Project ['date_created, 'parent_id]
        +- 'UnresolvedRelation [100k_usd], [], false
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
  	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
  	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
  	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
  	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
  	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
  	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
  	... 16 more
  
15:17:27.900649 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ba0eda5e-9d79-4ce6-aa1c-3f2f49e04bab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AF4A421C0>]}
15:17:27.901648 [error] [Thread-1  ]: 1 of 1 ERROR creating view model dbt.100kusd.................................... [[31mERROR[0m in 3.65s]
15:17:27.905631 [debug] [Thread-1  ]: Finished running node model.dbt_fivetran_1.100kusd
15:17:27.912633 [debug] [MainThread]: Acquiring new databricks connection "master"
15:17:27.913633 [debug] [MainThread]: On master: ROLLBACK
15:17:27.913633 [debug] [MainThread]: Opening a new connection, currently in state init
15:17:29.015226 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
15:17:29.016226 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
15:17:29.016226 [debug] [MainThread]: Spark adapter: NotImplemented: commit
15:17:29.017226 [debug] [MainThread]: On master: ROLLBACK
15:17:29.018226 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
15:17:29.018226 [debug] [MainThread]: On master: Close
15:17:30.142840 [info ] [MainThread]: 
15:17:30.145114 [info ] [MainThread]: Finished running 1 view model in 13.28s.
15:17:30.148158 [debug] [MainThread]: Connection 'master' was properly closed.
15:17:30.150161 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
15:17:30.151160 [debug] [MainThread]: Connection 'list_None_dbt' was properly closed.
15:17:30.151160 [debug] [MainThread]: Connection 'model.dbt_fivetran_1.100kusd' was properly closed.
15:17:30.186152 [info ] [MainThread]: 
15:17:30.194149 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
15:17:30.195149 [info ] [MainThread]: 
15:17:30.197176 [error] [MainThread]: [33mRuntime Error in model 100kusd (models\example\100kusd.sql)[0m
15:17:30.198153 [error] [MainThread]:   org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
15:17:30.200156 [error] [MainThread]:   'GlobalLimit 10
15:17:30.201151 [error] [MainThread]:   +- 'LocalLimit 10
15:17:30.202149 [error] [MainThread]:      +- 'Project ['date_created, 'parent_id]
15:17:30.205154 [error] [MainThread]:         +- 'UnresolvedRelation [100k_usd], [], false
15:17:30.210153 [error] [MainThread]:   
15:17:30.212149 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
15:17:30.213152 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
15:17:30.214151 [error] [MainThread]:   	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
15:17:30.215151 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
15:17:30.217150 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
15:17:30.218148 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
15:17:30.220149 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
15:17:30.221150 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
15:17:30.223166 [error] [MainThread]:   	at java.security.AccessController.doPrivileged(Native Method)
15:17:30.224152 [error] [MainThread]:   	at javax.security.auth.Subject.doAs(Subject.java:422)
15:17:30.226148 [error] [MainThread]:   	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
15:17:30.227149 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
15:17:30.229155 [error] [MainThread]:   	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
15:17:30.230149 [error] [MainThread]:   	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
15:17:30.234150 [error] [MainThread]:   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
15:17:30.236155 [error] [MainThread]:   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
15:17:30.244148 [error] [MainThread]:   	at java.lang.Thread.run(Thread.java:748)
15:17:30.246151 [error] [MainThread]:   Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
15:17:30.251149 [error] [MainThread]:   'GlobalLimit 10
15:17:30.253151 [error] [MainThread]:   +- 'LocalLimit 10
15:17:30.256161 [error] [MainThread]:      +- 'Project ['date_created, 'parent_id]
15:17:30.258150 [error] [MainThread]:         +- 'UnresolvedRelation [100k_usd], [], false
15:17:30.261153 [error] [MainThread]:   
15:17:30.263149 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
15:17:30.264149 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
15:17:30.266153 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
15:17:30.267153 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
15:17:30.268152 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
15:17:30.274149 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
15:17:30.275153 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
15:17:30.276152 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
15:17:30.277152 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
15:17:30.280150 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
15:17:30.282151 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
15:17:30.285150 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
15:17:30.289150 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
15:17:30.292151 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
15:17:30.307150 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
15:17:30.308150 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
15:17:30.310154 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
15:17:30.311151 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
15:17:30.312148 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
15:17:30.313148 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
15:17:30.315150 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
15:17:30.316153 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
15:17:30.317151 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
15:17:30.318150 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
15:17:30.320154 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
15:17:30.322151 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
15:17:30.328150 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
15:17:30.330150 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
15:17:30.332150 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
15:17:30.333154 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
15:17:30.334149 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
15:17:30.335149 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
15:17:30.336148 [error] [MainThread]:   	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
15:17:30.338149 [error] [MainThread]:   	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
15:17:30.341151 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
15:17:30.343150 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
15:17:30.344153 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
15:17:30.346152 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
15:17:30.347148 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
15:17:30.353169 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
15:17:30.359150 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
15:17:30.360152 [error] [MainThread]:   	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
15:17:30.362151 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
15:17:30.363149 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
15:17:30.364149 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
15:17:30.365151 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
15:17:30.367150 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
15:17:30.369154 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
15:17:30.372156 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
15:17:30.374147 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
15:17:30.379150 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
15:17:30.380148 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
15:17:30.381151 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
15:17:30.382151 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
15:17:30.384150 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
15:17:30.385154 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
15:17:30.388149 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
15:17:30.389153 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
15:17:30.391152 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
15:17:30.392149 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
15:17:30.393151 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
15:17:30.395151 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
15:17:30.396152 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
15:17:30.398152 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
15:17:30.407149 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
15:17:30.409149 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
15:17:30.410151 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
15:17:30.411149 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
15:17:30.413150 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
15:17:30.414151 [error] [MainThread]:   	... 16 more
15:17:30.418150 [error] [MainThread]:   
15:17:30.419153 [info ] [MainThread]: 
15:17:30.426149 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
15:17:30.427148 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AF47D86D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AF4820EB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AF4858520>]}


============================== 2022-06-21 16:06:03.210823 | 2a3ccbd4-d027-4088-88c2-eeebbcc2f838 ==============================
16:06:03.210823 [info ] [MainThread]: Running with dbt=1.0.5
16:06:03.210823 [debug] [MainThread]: running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=None, defer=None, event_buffer_size=None, exclude=None, fail_fast=None, full_refresh=False, log_cache_events=False, log_format=None, partial_parse=None, printer_width=None, profile=None, profiles_dir='C:\\Users\\DELL\\.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=['example.100kusd'], selector_name=None, send_anonymous_usage_stats=None, single_threaded=False, state=None, static_parser=None, target=None, threads=None, use_colors=None, use_experimental_parser=None, vars='{}', version_check=None, warn_error=None, which='run', write_json=None)
16:06:03.210823 [debug] [MainThread]: Tracking: tracking
16:06:03.213320 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002361FB23D00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002361FB23D90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002361FB23DC0>]}
16:06:03.433294 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
16:06:03.433294 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
16:06:03.433294 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2a3ccbd4-d027-4088-88c2-eeebbcc2f838', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002361FBF2BB0>]}
16:06:03.448926 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2a3ccbd4-d027-4088-88c2-eeebbcc2f838', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002361FBDAE50>]}
16:06:03.448926 [info ] [MainThread]: Found 3 models, 6 tests, 0 snapshots, 0 analyses, 218 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
16:06:03.462549 [info ] [MainThread]: 
16:06:03.463546 [debug] [MainThread]: Acquiring new databricks connection "master"
16:06:03.466544 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
16:06:03.467545 [debug] [ThreadPool]: Using databricks connection "list_schemas"
16:06:03.467545 [debug] [ThreadPool]: On list_schemas: GetSchemas(database=None, schema=None)
16:06:03.467545 [debug] [ThreadPool]: Opening a new connection, currently in state init
16:06:08.145919 [debug] [ThreadPool]: SQL status: OK in 4.68 seconds
16:06:08.161534 [debug] [ThreadPool]: On list_schemas: Close
16:06:09.296239 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_dbt"
16:06:09.349706 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
16:06:09.349706 [debug] [ThreadPool]: Using databricks connection "list_None_dbt"
16:06:09.349706 [debug] [ThreadPool]: On list_None_dbt: /* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "connection_name": "list_None_dbt"} */
show table extended in dbt like '*'
  
16:06:09.349706 [debug] [ThreadPool]: Opening a new connection, currently in state init
16:06:11.874553 [debug] [ThreadPool]: SQL status: OK in 2.52 seconds
16:06:11.890103 [debug] [ThreadPool]: On list_None_dbt: ROLLBACK
16:06:11.890103 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
16:06:11.890103 [debug] [ThreadPool]: On list_None_dbt: Close
16:06:12.946630 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
16:06:12.946630 [debug] [MainThread]: Spark adapter: NotImplemented: commit
16:06:12.946630 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
16:06:12.946630 [info ] [MainThread]: 
16:06:12.984726 [debug] [Thread-1  ]: Began running node model.dbt_fivetran_1.100kusd
16:06:12.984726 [info ] [Thread-1  ]: 1 of 1 START view model dbt.100kusd............................................. [RUN]
16:06:13.001927 [debug] [Thread-1  ]: Acquiring new databricks connection "model.dbt_fivetran_1.100kusd"
16:06:13.002927 [debug] [Thread-1  ]: Began compiling node model.dbt_fivetran_1.100kusd
16:06:13.002927 [debug] [Thread-1  ]: Compiling model.dbt_fivetran_1.100kusd
16:06:13.005925 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_fivetran_1.100kusd"
16:06:13.006923 [debug] [Thread-1  ]: finished collecting timing info
16:06:13.006923 [debug] [Thread-1  ]: Began executing node model.dbt_fivetran_1.100kusd
16:06:13.115884 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_fivetran_1.100kusd"
16:06:13.117888 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
16:06:13.117888 [debug] [Thread-1  ]: Using databricks connection "model.dbt_fivetran_1.100kusd"
16:06:13.117888 [debug] [Thread-1  ]: On model.dbt_fivetran_1.100kusd: /* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "node_id": "model.dbt_fivetran_1.100kusd"} */
create or replace view dbt.100kusd
  
  
  as
    select date_created,parent_id from 100k_usd limit 10;

16:06:13.118888 [debug] [Thread-1  ]: Opening a new connection, currently in state init
16:06:15.430057 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "node_id": "model.dbt_fivetran_1.100kusd"} */
create or replace view dbt.100kusd
  
  
  as
    select date_created,parent_id from 100k_usd limit 10;

16:06:15.430057 [debug] [Thread-1  ]: Databricks adapter: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
'GlobalLimit 10
+- 'LocalLimit 10
   +- 'Project ['date_created, 'parent_id]
      +- 'UnresolvedRelation [100k_usd], [], false

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
'GlobalLimit 10
+- 'LocalLimit 10
   +- 'Project ['date_created, 'parent_id]
      +- 'UnresolvedRelation [100k_usd], [], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
	... 16 more

16:06:15.430057 [debug] [Thread-1  ]: finished collecting timing info
16:06:15.430057 [debug] [Thread-1  ]: On model.dbt_fivetran_1.100kusd: ROLLBACK
16:06:15.430057 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
16:06:15.430057 [debug] [Thread-1  ]: On model.dbt_fivetran_1.100kusd: Close
16:06:16.486292 [debug] [Thread-1  ]: Runtime Error in model 100kusd (models\example\100kusd.sql)
  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
  'GlobalLimit 10
  +- 'LocalLimit 10
     +- 'Project ['date_created, 'parent_id]
        +- 'UnresolvedRelation [100k_usd], [], false
  
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
  	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
  	at java.security.AccessController.doPrivileged(Native Method)
  	at javax.security.auth.Subject.doAs(Subject.java:422)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
  	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	at java.lang.Thread.run(Thread.java:748)
  Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
  'GlobalLimit 10
  +- 'LocalLimit 10
     +- 'Project ['date_created, 'parent_id]
        +- 'UnresolvedRelation [100k_usd], [], false
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
  	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
  	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
  	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
  	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
  	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
  	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
  	... 16 more
  
16:06:16.486292 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2a3ccbd4-d027-4088-88c2-eeebbcc2f838', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002361FD27160>]}
16:06:16.486292 [error] [Thread-1  ]: 1 of 1 ERROR creating view model dbt.100kusd.................................... [[31mERROR[0m in 3.48s]
16:06:16.501844 [debug] [Thread-1  ]: Finished running node model.dbt_fivetran_1.100kusd
16:06:16.501844 [debug] [MainThread]: Acquiring new databricks connection "master"
16:06:16.501844 [debug] [MainThread]: On master: ROLLBACK
16:06:16.501844 [debug] [MainThread]: Opening a new connection, currently in state init
16:06:17.662604 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
16:06:17.662604 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
16:06:17.662604 [debug] [MainThread]: Spark adapter: NotImplemented: commit
16:06:17.662604 [debug] [MainThread]: On master: ROLLBACK
16:06:17.662604 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
16:06:17.662604 [debug] [MainThread]: On master: Close
16:06:18.706705 [info ] [MainThread]: 
16:06:18.708702 [info ] [MainThread]: Finished running 1 view model in 15.24s.
16:06:18.711864 [debug] [MainThread]: Connection 'master' was properly closed.
16:06:18.714626 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
16:06:18.715622 [debug] [MainThread]: Connection 'list_None_dbt' was properly closed.
16:06:18.716635 [debug] [MainThread]: Connection 'model.dbt_fivetran_1.100kusd' was properly closed.
16:06:18.749124 [info ] [MainThread]: 
16:06:18.750118 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
16:06:18.752112 [info ] [MainThread]: 
16:06:18.753138 [error] [MainThread]: [33mRuntime Error in model 100kusd (models\example\100kusd.sql)[0m
16:06:18.756114 [error] [MainThread]:   org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
16:06:18.758123 [error] [MainThread]:   'GlobalLimit 10
16:06:18.761117 [error] [MainThread]:   +- 'LocalLimit 10
16:06:18.765117 [error] [MainThread]:      +- 'Project ['date_created, 'parent_id]
16:06:18.766112 [error] [MainThread]:         +- 'UnresolvedRelation [100k_usd], [], false
16:06:18.767113 [error] [MainThread]:   
16:06:18.769114 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
16:06:18.772120 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
16:06:18.774116 [error] [MainThread]:   	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
16:06:18.775114 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
16:06:18.783118 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
16:06:18.784113 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
16:06:18.786115 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
16:06:18.787112 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
16:06:18.788116 [error] [MainThread]:   	at java.security.AccessController.doPrivileged(Native Method)
16:06:18.790114 [error] [MainThread]:   	at javax.security.auth.Subject.doAs(Subject.java:422)
16:06:18.791115 [error] [MainThread]:   	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
16:06:18.792122 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
16:06:18.794115 [error] [MainThread]:   	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
16:06:18.796115 [error] [MainThread]:   	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
16:06:18.798117 [error] [MainThread]:   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
16:06:18.800119 [error] [MainThread]:   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
16:06:18.802122 [error] [MainThread]:   	at java.lang.Thread.run(Thread.java:748)
16:06:18.804113 [error] [MainThread]:   Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: 100k_usd; line 6 pos 39;
16:06:18.805115 [error] [MainThread]:   'GlobalLimit 10
16:06:18.811117 [error] [MainThread]:   +- 'LocalLimit 10
16:06:18.813115 [error] [MainThread]:      +- 'Project ['date_created, 'parent_id]
16:06:18.815121 [error] [MainThread]:         +- 'UnresolvedRelation [100k_usd], [], false
16:06:18.816115 [error] [MainThread]:   
16:06:18.820138 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
16:06:18.822117 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
16:06:18.824114 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
16:06:18.825118 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
16:06:18.826114 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
16:06:18.828121 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
16:06:18.830116 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
16:06:18.831116 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
16:06:18.832113 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
16:06:18.833113 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
16:06:18.837115 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
16:06:18.838112 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
16:06:18.840114 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
16:06:18.842115 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
16:06:18.843114 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
16:06:18.844115 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
16:06:18.845118 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
16:06:18.847118 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
16:06:18.848113 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
16:06:18.849114 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
16:06:18.850115 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
16:06:18.852115 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
16:06:18.855120 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
16:06:18.857119 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
16:06:18.858114 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
16:06:18.859114 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
16:06:18.865115 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
16:06:18.866113 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
16:06:18.868115 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
16:06:18.869116 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
16:06:18.871116 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
16:06:18.872114 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
16:06:18.873117 [error] [MainThread]:   	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
16:06:18.874113 [error] [MainThread]:   	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
16:06:18.875113 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
16:06:18.876112 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
16:06:18.879116 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
16:06:18.897113 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
16:06:18.914114 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
16:06:18.916114 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
16:06:18.920114 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
16:06:18.926117 [error] [MainThread]:   	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
16:06:18.951112 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
16:06:18.956114 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
16:06:18.969116 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
16:06:18.971113 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
16:06:18.972118 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
16:06:18.974114 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
16:06:18.975118 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
16:06:18.976113 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
16:06:18.979115 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
16:06:18.981114 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
16:06:18.982114 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
16:06:18.983119 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
16:06:18.984113 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
16:06:18.986114 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
16:06:18.988112 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
16:06:18.997114 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
16:06:18.998114 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
16:06:19.000112 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
16:06:19.001113 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
16:06:19.002114 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
16:06:19.003113 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
16:06:19.004113 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
16:06:19.005113 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
16:06:19.006115 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
16:06:19.007115 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
16:06:19.009117 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
16:06:19.011116 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
16:06:19.013117 [error] [MainThread]:   	... 16 more
16:06:19.014118 [error] [MainThread]:   
16:06:19.015117 [info ] [MainThread]: 
16:06:19.017116 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
16:06:19.018115 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002361FB3C700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002361FACFB80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002361FBE24C0>]}


============================== 2022-06-22 15:03:35.391242 | ab3cb658-2bde-49e2-b0f6-177e3aec4972 ==============================
15:03:35.391242 [info ] [MainThread]: Running with dbt=1.0.5
15:03:35.392246 [debug] [MainThread]: running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=None, defer=None, event_buffer_size=None, exclude=None, fail_fast=None, full_refresh=False, log_cache_events=False, log_format=None, partial_parse=None, printer_width=None, profile=None, profiles_dir='C:\\Users\\DELL\\.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=['example.100kusd'], selector_name=None, send_anonymous_usage_stats=None, single_threaded=False, state=None, static_parser=None, target=None, threads=None, use_colors=None, use_experimental_parser=None, vars='{}', version_check=None, warn_error=None, which='run', write_json=None)
15:03:35.393246 [debug] [MainThread]: Tracking: tracking
15:03:35.396247 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F52FBB3BE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F52FBB3C70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F52FBB3CA0>]}
15:03:35.561246 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
15:03:35.562246 [debug] [MainThread]: Partial parsing: updated file: dbt_fivetran_1://models\example\100kusd.sql
15:03:35.578244 [debug] [MainThread]: 1699: static parser successfully parsed example\100kusd.sql
15:03:35.640491 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ab3cb658-2bde-49e2-b0f6-177e3aec4972', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F52FD07160>]}
15:03:35.650493 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ab3cb658-2bde-49e2-b0f6-177e3aec4972', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F52FC6BCD0>]}
15:03:35.650493 [info ] [MainThread]: Found 3 models, 6 tests, 0 snapshots, 0 analyses, 218 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
15:03:35.653492 [info ] [MainThread]: 
15:03:35.654483 [debug] [MainThread]: Acquiring new databricks connection "master"
15:03:35.657470 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
15:03:35.658498 [debug] [ThreadPool]: Using databricks connection "list_schemas"
15:03:35.659491 [debug] [ThreadPool]: On list_schemas: GetSchemas(database=None, schema=None)
15:03:35.659491 [debug] [ThreadPool]: Opening a new connection, currently in state init
15:03:43.090787 [debug] [ThreadPool]: SQL status: OK in 7.43 seconds
15:03:43.139570 [debug] [ThreadPool]: On list_schemas: Close
15:03:52.008011 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_dbt"
15:03:52.039003 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
15:03:52.039003 [debug] [ThreadPool]: Using databricks connection "list_None_dbt"
15:03:52.040003 [debug] [ThreadPool]: On list_None_dbt: /* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "connection_name": "list_None_dbt"} */
show table extended in dbt like '*'
  
15:03:52.040003 [debug] [ThreadPool]: Opening a new connection, currently in state init
15:03:56.683667 [debug] [ThreadPool]: SQL status: OK in 4.64 seconds
15:03:56.696026 [debug] [ThreadPool]: On list_None_dbt: ROLLBACK
15:03:56.697028 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
15:03:56.698028 [debug] [ThreadPool]: On list_None_dbt: Close
15:03:57.841135 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
15:03:57.842179 [debug] [MainThread]: Spark adapter: NotImplemented: commit
15:03:57.844178 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
15:03:57.846023 [info ] [MainThread]: 
15:03:57.876059 [debug] [Thread-1  ]: Began running node model.dbt_fivetran_1.100kusd
15:03:57.876059 [info ] [Thread-1  ]: 1 of 1 START view model dbt.100kusd............................................. [RUN]
15:03:57.878055 [debug] [Thread-1  ]: Acquiring new databricks connection "model.dbt_fivetran_1.100kusd"
15:03:57.879060 [debug] [Thread-1  ]: Began compiling node model.dbt_fivetran_1.100kusd
15:03:57.879060 [debug] [Thread-1  ]: Compiling model.dbt_fivetran_1.100kusd
15:03:57.883070 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_fivetran_1.100kusd"
15:03:57.884059 [debug] [Thread-1  ]: finished collecting timing info
15:03:57.884059 [debug] [Thread-1  ]: Began executing node model.dbt_fivetran_1.100kusd
15:03:57.942061 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_fivetran_1.100kusd"
15:03:57.943059 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
15:03:57.944059 [debug] [Thread-1  ]: Using databricks connection "model.dbt_fivetran_1.100kusd"
15:03:57.944059 [debug] [Thread-1  ]: On model.dbt_fivetran_1.100kusd: /* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "node_id": "model.dbt_fivetran_1.100kusd"} */
create or replace view dbt.100kusd
  
  
  as
    select date_created,access_type_extid from access_type limit 10;

15:03:57.944059 [debug] [Thread-1  ]: Opening a new connection, currently in state init
15:04:00.548405 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "node_id": "model.dbt_fivetran_1.100kusd"} */
create or replace view dbt.100kusd
  
  
  as
    select date_created,access_type_extid from access_type limit 10;

15:04:00.549404 [debug] [Thread-1  ]: Databricks adapter: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: access_type; line 6 pos 47;
'GlobalLimit 10
+- 'LocalLimit 10
   +- 'Project ['date_created, 'access_type_extid]
      +- 'UnresolvedRelation [access_type], [], false

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: access_type; line 6 pos 47;
'GlobalLimit 10
+- 'LocalLimit 10
   +- 'Project ['date_created, 'access_type_extid]
      +- 'UnresolvedRelation [access_type], [], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
	... 16 more

15:04:00.551406 [debug] [Thread-1  ]: finished collecting timing info
15:04:00.552405 [debug] [Thread-1  ]: On model.dbt_fivetran_1.100kusd: ROLLBACK
15:04:00.552405 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
15:04:00.553405 [debug] [Thread-1  ]: On model.dbt_fivetran_1.100kusd: Close
15:04:01.675460 [debug] [Thread-1  ]: Runtime Error in model 100kusd (models\example\100kusd.sql)
  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: access_type; line 6 pos 47;
  'GlobalLimit 10
  +- 'LocalLimit 10
     +- 'Project ['date_created, 'access_type_extid]
        +- 'UnresolvedRelation [access_type], [], false
  
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
  	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
  	at java.security.AccessController.doPrivileged(Native Method)
  	at javax.security.auth.Subject.doAs(Subject.java:422)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
  	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	at java.lang.Thread.run(Thread.java:748)
  Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: access_type; line 6 pos 47;
  'GlobalLimit 10
  +- 'LocalLimit 10
     +- 'Project ['date_created, 'access_type_extid]
        +- 'UnresolvedRelation [access_type], [], false
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
  	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
  	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
  	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
  	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
  	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
  	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
  	... 16 more
  
15:04:01.676454 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ab3cb658-2bde-49e2-b0f6-177e3aec4972', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F52FDED3D0>]}
15:04:01.678453 [error] [Thread-1  ]: 1 of 1 ERROR creating view model dbt.100kusd.................................... [[31mERROR[0m in 3.80s]
15:04:01.680454 [debug] [Thread-1  ]: Finished running node model.dbt_fivetran_1.100kusd
15:04:01.682454 [debug] [MainThread]: Acquiring new databricks connection "master"
15:04:01.683453 [debug] [MainThread]: On master: ROLLBACK
15:04:01.684456 [debug] [MainThread]: Opening a new connection, currently in state init
15:04:02.880458 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
15:04:02.880458 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
15:04:02.881458 [debug] [MainThread]: Spark adapter: NotImplemented: commit
15:04:02.881458 [debug] [MainThread]: On master: ROLLBACK
15:04:02.881458 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
15:04:02.881458 [debug] [MainThread]: On master: Close
15:04:04.105459 [info ] [MainThread]: 
15:04:04.106459 [info ] [MainThread]: Finished running 1 view model in 28.45s.
15:04:04.107454 [debug] [MainThread]: Connection 'master' was properly closed.
15:04:04.108456 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
15:04:04.108456 [debug] [MainThread]: Connection 'list_None_dbt' was properly closed.
15:04:04.109460 [debug] [MainThread]: Connection 'model.dbt_fivetran_1.100kusd' was properly closed.
15:04:04.125457 [info ] [MainThread]: 
15:04:04.126457 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
15:04:04.128453 [info ] [MainThread]: 
15:04:04.129457 [error] [MainThread]: [33mRuntime Error in model 100kusd (models\example\100kusd.sql)[0m
15:04:04.131459 [error] [MainThread]:   org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: access_type; line 6 pos 47;
15:04:04.133459 [error] [MainThread]:   'GlobalLimit 10
15:04:04.135469 [error] [MainThread]:   +- 'LocalLimit 10
15:04:04.137464 [error] [MainThread]:      +- 'Project ['date_created, 'access_type_extid]
15:04:04.139457 [error] [MainThread]:         +- 'UnresolvedRelation [access_type], [], false
15:04:04.140456 [error] [MainThread]:   
15:04:04.142457 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
15:04:04.143457 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
15:04:04.145454 [error] [MainThread]:   	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
15:04:04.146455 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
15:04:04.147457 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
15:04:04.149454 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
15:04:04.150456 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
15:04:04.152454 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
15:04:04.153458 [error] [MainThread]:   	at java.security.AccessController.doPrivileged(Native Method)
15:04:04.155457 [error] [MainThread]:   	at javax.security.auth.Subject.doAs(Subject.java:422)
15:04:04.157459 [error] [MainThread]:   	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
15:04:04.160462 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
15:04:04.161457 [error] [MainThread]:   	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
15:04:04.162457 [error] [MainThread]:   	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
15:04:04.164457 [error] [MainThread]:   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
15:04:04.165458 [error] [MainThread]:   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
15:04:04.166458 [error] [MainThread]:   	at java.lang.Thread.run(Thread.java:748)
15:04:04.167459 [error] [MainThread]:   Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: access_type; line 6 pos 47;
15:04:04.169457 [error] [MainThread]:   'GlobalLimit 10
15:04:04.170458 [error] [MainThread]:   +- 'LocalLimit 10
15:04:04.172456 [error] [MainThread]:      +- 'Project ['date_created, 'access_type_extid]
15:04:04.174477 [error] [MainThread]:         +- 'UnresolvedRelation [access_type], [], false
15:04:04.175458 [error] [MainThread]:   
15:04:04.178456 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
15:04:04.181455 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
15:04:04.184456 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
15:04:04.189456 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
15:04:04.196465 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
15:04:04.200456 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
15:04:04.201454 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
15:04:04.203453 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
15:04:04.206457 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
15:04:04.208456 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
15:04:04.209454 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
15:04:04.210458 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
15:04:04.212455 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
15:04:04.215458 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
15:04:04.218456 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
15:04:04.220456 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
15:04:04.222456 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
15:04:04.225455 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
15:04:04.226455 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
15:04:04.228455 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
15:04:04.229460 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
15:04:04.230456 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
15:04:04.233457 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
15:04:04.234454 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
15:04:04.237455 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
15:04:04.238458 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
15:04:04.240461 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
15:04:04.242455 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
15:04:04.243459 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
15:04:04.245457 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
15:04:04.247455 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
15:04:04.251481 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
15:04:04.253457 [error] [MainThread]:   	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
15:04:04.255464 [error] [MainThread]:   	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
15:04:04.257454 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
15:04:04.258454 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
15:04:04.259455 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
15:04:04.261459 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
15:04:04.263457 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
15:04:04.265452 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
15:04:04.266457 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
15:04:04.271455 [error] [MainThread]:   	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
15:04:04.273465 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
15:04:04.276457 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
15:04:04.279456 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
15:04:04.280459 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
15:04:04.282456 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
15:04:04.283460 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
15:04:04.285456 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
15:04:04.286457 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
15:04:04.288455 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
15:04:04.290462 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
15:04:04.292454 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
15:04:04.293461 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
15:04:04.295455 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
15:04:04.298458 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
15:04:04.302456 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
15:04:04.304458 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
15:04:04.307467 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
15:04:04.311455 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
15:04:04.312457 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
15:04:04.314460 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
15:04:04.316456 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
15:04:04.317459 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
15:04:04.321467 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
15:04:04.325456 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
15:04:04.327457 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
15:04:04.328455 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
15:04:04.330453 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
15:04:04.332455 [error] [MainThread]:   	... 16 more
15:04:04.333458 [error] [MainThread]:   
15:04:04.334455 [info ] [MainThread]: 
15:04:04.336455 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
15:04:04.338456 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F52FC6BAC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F52FC6BA00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F52FC5CB20>]}


============================== 2022-06-23 15:07:08.255146 | 6091f8c2-54e5-4a7d-8bc0-a7b9e4677d10 ==============================
15:07:08.255146 [info ] [MainThread]: Running with dbt=1.0.5
15:07:08.255146 [debug] [MainThread]: running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=None, defer=None, event_buffer_size=None, exclude=None, fail_fast=None, full_refresh=False, log_cache_events=False, log_format=None, partial_parse=None, printer_width=None, profile=None, profiles_dir='C:\\Users\\DELL\\.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=['example.100kusd'], selector_name=None, send_anonymous_usage_stats=None, single_threaded=False, state=None, static_parser=None, target=None, threads=None, use_colors=None, use_experimental_parser=None, vars='{}', version_check=None, warn_error=None, which='run', write_json=None)
15:07:08.255146 [debug] [MainThread]: Tracking: tracking
15:07:08.255146 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC749B1D60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC749B1DF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC749B1E20>]}
15:07:08.427503 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
15:07:08.427503 [debug] [MainThread]: Partial parsing: updated file: dbt_fivetran_1://models\example\100kusd.sql
15:07:08.444832 [debug] [MainThread]: 1699: static parser successfully parsed example\100kusd.sql
15:07:08.503641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6091f8c2-54e5-4a7d-8bc0-a7b9e4677d10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC74B085B0>]}
15:07:08.512363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6091f8c2-54e5-4a7d-8bc0-a7b9e4677d10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC74997C40>]}
15:07:08.512363 [info ] [MainThread]: Found 3 models, 6 tests, 0 snapshots, 0 analyses, 218 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
15:07:08.512363 [info ] [MainThread]: 
15:07:08.512363 [debug] [MainThread]: Acquiring new databricks connection "master"
15:07:08.512363 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
15:07:08.520338 [debug] [ThreadPool]: Using databricks connection "list_schemas"
15:07:08.520338 [debug] [ThreadPool]: On list_schemas: GetSchemas(database=None, schema=None)
15:07:08.520338 [debug] [ThreadPool]: Opening a new connection, currently in state init
15:07:11.683299 [debug] [ThreadPool]: SQL status: OK in 3.16 seconds
15:07:11.723824 [debug] [ThreadPool]: On list_schemas: Close
15:07:21.367420 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_dbt"
15:07:21.392018 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
15:07:21.392018 [debug] [ThreadPool]: Using databricks connection "list_None_dbt"
15:07:21.392018 [debug] [ThreadPool]: On list_None_dbt: /* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "connection_name": "list_None_dbt"} */
show table extended in dbt like '*'
  
15:07:21.392018 [debug] [ThreadPool]: Opening a new connection, currently in state init
15:07:24.864426 [debug] [ThreadPool]: SQL status: OK in 3.47 seconds
15:07:24.881415 [debug] [ThreadPool]: On list_None_dbt: ROLLBACK
15:07:24.881415 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
15:07:24.881415 [debug] [ThreadPool]: On list_None_dbt: Close
15:07:26.081929 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
15:07:26.081929 [debug] [MainThread]: Spark adapter: NotImplemented: commit
15:07:26.081929 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
15:07:26.081929 [info ] [MainThread]: 
15:07:26.113943 [debug] [Thread-1  ]: Began running node model.dbt_fivetran_1.100kusd
15:07:26.121929 [info ] [Thread-1  ]: 1 of 1 START view model dbt.100kusd............................................. [RUN]
15:07:26.121929 [debug] [Thread-1  ]: Acquiring new databricks connection "model.dbt_fivetran_1.100kusd"
15:07:26.121929 [debug] [Thread-1  ]: Began compiling node model.dbt_fivetran_1.100kusd
15:07:26.129930 [debug] [Thread-1  ]: Compiling model.dbt_fivetran_1.100kusd
15:07:26.137936 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_fivetran_1.100kusd"
15:07:26.137936 [debug] [Thread-1  ]: finished collecting timing info
15:07:26.137936 [debug] [Thread-1  ]: Began executing node model.dbt_fivetran_1.100kusd
15:07:26.217917 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_fivetran_1.100kusd"
15:07:26.217917 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
15:07:26.217917 [debug] [Thread-1  ]: Using databricks connection "model.dbt_fivetran_1.100kusd"
15:07:26.217917 [debug] [Thread-1  ]: On model.dbt_fivetran_1.100kusd: /* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "node_id": "model.dbt_fivetran_1.100kusd"} */
create or replace view dbt.100kusd
  
  
  as
    select date_created,approval_level_extid from approval_level limit 10;

15:07:26.217917 [debug] [Thread-1  ]: Opening a new connection, currently in state init
15:07:29.067241 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "node_id": "model.dbt_fivetran_1.100kusd"} */
create or replace view dbt.100kusd
  
  
  as
    select date_created,approval_level_extid from approval_level limit 10;

15:07:29.067241 [debug] [Thread-1  ]: Databricks adapter: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: approval_level; line 6 pos 50;
'GlobalLimit 10
+- 'LocalLimit 10
   +- 'Project ['date_created, 'approval_level_extid]
      +- 'UnresolvedRelation [approval_level], [], false

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: approval_level; line 6 pos 50;
'GlobalLimit 10
+- 'LocalLimit 10
   +- 'Project ['date_created, 'approval_level_extid]
      +- 'UnresolvedRelation [approval_level], [], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
	... 16 more

15:07:29.071824 [debug] [Thread-1  ]: finished collecting timing info
15:07:29.071824 [debug] [Thread-1  ]: On model.dbt_fivetran_1.100kusd: ROLLBACK
15:07:29.071824 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
15:07:29.071824 [debug] [Thread-1  ]: On model.dbt_fivetran_1.100kusd: Close
15:07:30.197504 [debug] [Thread-1  ]: Runtime Error in model 100kusd (models\example\100kusd.sql)
  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: approval_level; line 6 pos 50;
  'GlobalLimit 10
  +- 'LocalLimit 10
     +- 'Project ['date_created, 'approval_level_extid]
        +- 'UnresolvedRelation [approval_level], [], false
  
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
  	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
  	at java.security.AccessController.doPrivileged(Native Method)
  	at javax.security.auth.Subject.doAs(Subject.java:422)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
  	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	at java.lang.Thread.run(Thread.java:748)
  Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: approval_level; line 6 pos 50;
  'GlobalLimit 10
  +- 'LocalLimit 10
     +- 'Project ['date_created, 'approval_level_extid]
        +- 'UnresolvedRelation [approval_level], [], false
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
  	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
  	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
  	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
  	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
  	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
  	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
  	... 16 more
  
15:07:30.197504 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6091f8c2-54e5-4a7d-8bc0-a7b9e4677d10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC74BF2C40>]}
15:07:30.197504 [error] [Thread-1  ]: 1 of 1 ERROR creating view model dbt.100kusd.................................... [[31mERROR[0m in 4.08s]
15:07:30.206857 [debug] [Thread-1  ]: Finished running node model.dbt_fivetran_1.100kusd
15:07:30.207471 [debug] [MainThread]: Acquiring new databricks connection "master"
15:07:30.207471 [debug] [MainThread]: On master: ROLLBACK
15:07:30.207471 [debug] [MainThread]: Opening a new connection, currently in state init
15:07:31.409303 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
15:07:31.409303 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
15:07:31.409303 [debug] [MainThread]: Spark adapter: NotImplemented: commit
15:07:31.417294 [debug] [MainThread]: On master: ROLLBACK
15:07:31.417294 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
15:07:31.417294 [debug] [MainThread]: On master: Close
15:07:32.727468 [info ] [MainThread]: 
15:07:32.735483 [info ] [MainThread]: Finished running 1 view model in 24.22s.
15:07:32.738996 [debug] [MainThread]: Connection 'master' was properly closed.
15:07:32.738996 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
15:07:32.738996 [debug] [MainThread]: Connection 'list_None_dbt' was properly closed.
15:07:32.738996 [debug] [MainThread]: Connection 'model.dbt_fivetran_1.100kusd' was properly closed.
15:07:32.778982 [info ] [MainThread]: 
15:07:32.786986 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
15:07:32.786986 [info ] [MainThread]: 
15:07:32.786986 [error] [MainThread]: [33mRuntime Error in model 100kusd (models\example\100kusd.sql)[0m
15:07:32.794977 [error] [MainThread]:   org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: approval_level; line 6 pos 50;
15:07:32.794977 [error] [MainThread]:   'GlobalLimit 10
15:07:32.794977 [error] [MainThread]:   +- 'LocalLimit 10
15:07:32.794977 [error] [MainThread]:      +- 'Project ['date_created, 'approval_level_extid]
15:07:32.794977 [error] [MainThread]:         +- 'UnresolvedRelation [approval_level], [], false
15:07:32.794977 [error] [MainThread]:   
15:07:32.802973 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
15:07:32.802973 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
15:07:32.802973 [error] [MainThread]:   	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
15:07:32.802973 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
15:07:32.810982 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
15:07:32.810982 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
15:07:32.810982 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
15:07:32.818976 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
15:07:32.818976 [error] [MainThread]:   	at java.security.AccessController.doPrivileged(Native Method)
15:07:32.818976 [error] [MainThread]:   	at javax.security.auth.Subject.doAs(Subject.java:422)
15:07:32.818976 [error] [MainThread]:   	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
15:07:32.818976 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
15:07:32.826976 [error] [MainThread]:   	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
15:07:32.826976 [error] [MainThread]:   	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
15:07:32.826976 [error] [MainThread]:   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
15:07:32.834980 [error] [MainThread]:   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
15:07:32.834980 [error] [MainThread]:   	at java.lang.Thread.run(Thread.java:748)
15:07:32.834980 [error] [MainThread]:   Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: approval_level; line 6 pos 50;
15:07:32.834980 [error] [MainThread]:   'GlobalLimit 10
15:07:32.834980 [error] [MainThread]:   +- 'LocalLimit 10
15:07:32.842976 [error] [MainThread]:      +- 'Project ['date_created, 'approval_level_extid]
15:07:32.842976 [error] [MainThread]:         +- 'UnresolvedRelation [approval_level], [], false
15:07:32.842976 [error] [MainThread]:   
15:07:32.842976 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
15:07:32.850975 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
15:07:32.850975 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
15:07:32.850975 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
15:07:32.850975 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
15:07:32.850975 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
15:07:32.858975 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
15:07:32.858975 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
15:07:32.858975 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
15:07:32.858975 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
15:07:32.858975 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
15:07:32.866974 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
15:07:32.866974 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
15:07:32.866974 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
15:07:32.866974 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
15:07:32.866974 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
15:07:32.874974 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
15:07:32.874974 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
15:07:32.874974 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
15:07:32.874974 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
15:07:32.874974 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
15:07:32.882976 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
15:07:32.882976 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
15:07:32.882976 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
15:07:32.882976 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
15:07:32.882976 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
15:07:32.890977 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
15:07:32.890977 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
15:07:32.890977 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
15:07:32.890977 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
15:07:32.898977 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
15:07:32.898977 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
15:07:32.898977 [error] [MainThread]:   	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
15:07:32.898977 [error] [MainThread]:   	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
15:07:32.898977 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
15:07:32.898977 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
15:07:32.906976 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
15:07:32.906976 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
15:07:32.906976 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
15:07:32.906976 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
15:07:32.906976 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
15:07:32.914987 [error] [MainThread]:   	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
15:07:32.914987 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
15:07:32.914987 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
15:07:32.914987 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
15:07:32.922974 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
15:07:32.922974 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
15:07:32.922974 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
15:07:32.930973 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
15:07:32.930973 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
15:07:32.930973 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
15:07:32.930973 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
15:07:32.930973 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
15:07:32.930973 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
15:07:32.938973 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
15:07:32.938973 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
15:07:32.938973 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
15:07:32.938973 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
15:07:32.938973 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
15:07:32.938973 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
15:07:32.938973 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
15:07:32.946973 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
15:07:32.946973 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
15:07:32.946973 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
15:07:32.954980 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
15:07:32.954980 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
15:07:32.954980 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
15:07:32.954980 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
15:07:32.954980 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
15:07:32.962973 [error] [MainThread]:   	... 16 more
15:07:32.962973 [error] [MainThread]:   
15:07:32.962973 [info ] [MainThread]: 
15:07:32.962973 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
15:07:32.962973 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC74A706A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC74A5BD00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC74A5BCD0>]}


============================== 2022-06-24 07:21:28.178492 | b0c76c9c-4ac9-469e-aebd-e15a1650f2fb ==============================
07:21:28.178492 [info ] [MainThread]: Running with dbt=1.0.5
07:21:28.178492 [debug] [MainThread]: running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=None, defer=None, event_buffer_size=None, exclude=None, fail_fast=None, full_refresh=False, log_cache_events=False, log_format=None, partial_parse=None, printer_width=None, profile=None, profiles_dir='C:\\Users\\DELL\\.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=['example.4599_system_note'], selector_name=None, send_anonymous_usage_stats=None, single_threaded=False, state=None, static_parser=None, target=None, threads=None, use_colors=None, use_experimental_parser=None, vars='{}', version_check=None, warn_error=None, which='run', write_json=None)
07:21:28.186500 [debug] [MainThread]: Tracking: tracking
07:21:28.186500 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F092052D00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F092052D90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F092052DC0>]}
07:21:28.346490 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
07:21:28.346490 [debug] [MainThread]: Partial parsing: added file: dbt_fivetran_1://models\example\4599_system_note.sql
07:21:28.362491 [debug] [MainThread]: 1699: static parser successfully parsed example\4599_system_note.sql
07:21:28.402495 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b0c76c9c-4ac9-469e-aebd-e15a1650f2fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0921AAD00>]}
07:21:28.410516 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b0c76c9c-4ac9-469e-aebd-e15a1650f2fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0920368E0>]}
07:21:28.410516 [info ] [MainThread]: Found 4 models, 6 tests, 0 snapshots, 0 analyses, 218 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
07:21:28.410516 [info ] [MainThread]: 
07:21:28.410516 [debug] [MainThread]: Acquiring new databricks connection "master"
07:21:28.418498 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
07:21:28.418498 [debug] [ThreadPool]: Using databricks connection "list_schemas"
07:21:28.418498 [debug] [ThreadPool]: On list_schemas: GetSchemas(database=None, schema=None)
07:21:28.418498 [debug] [ThreadPool]: Opening a new connection, currently in state init
07:21:33.330650 [debug] [ThreadPool]: SQL status: OK in 4.91 seconds
07:21:33.370649 [debug] [ThreadPool]: On list_schemas: Close
07:21:43.340625 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_dbt"
07:21:43.356653 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
07:21:43.356653 [debug] [ThreadPool]: Using databricks connection "list_None_dbt"
07:21:43.356653 [debug] [ThreadPool]: On list_None_dbt: /* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "connection_name": "list_None_dbt"} */
show table extended in dbt like '*'
  
07:21:43.356653 [debug] [ThreadPool]: Opening a new connection, currently in state init
07:21:46.239534 [debug] [ThreadPool]: SQL status: OK in 2.88 seconds
07:21:46.255523 [debug] [ThreadPool]: On list_None_dbt: ROLLBACK
07:21:46.255523 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
07:21:46.255523 [debug] [ThreadPool]: On list_None_dbt: Close
07:21:47.410028 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
07:21:47.410028 [debug] [MainThread]: Spark adapter: NotImplemented: commit
07:21:47.418032 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
07:21:47.418032 [info ] [MainThread]: 
07:21:47.441983 [debug] [Thread-1  ]: Began running node model.dbt_fivetran_1.4599_system_note
07:21:47.441983 [info ] [Thread-1  ]: 1 of 1 START view model dbt.4599_system_note.................................... [RUN]
07:21:47.441983 [debug] [Thread-1  ]: Acquiring new databricks connection "model.dbt_fivetran_1.4599_system_note"
07:21:47.441983 [debug] [Thread-1  ]: Began compiling node model.dbt_fivetran_1.4599_system_note
07:21:47.441983 [debug] [Thread-1  ]: Compiling model.dbt_fivetran_1.4599_system_note
07:21:47.449999 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_fivetran_1.4599_system_note"
07:21:47.449999 [debug] [Thread-1  ]: finished collecting timing info
07:21:47.449999 [debug] [Thread-1  ]: Began executing node model.dbt_fivetran_1.4599_system_note
07:21:47.553978 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_fivetran_1.4599_system_note"
07:21:47.553978 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
07:21:47.553978 [debug] [Thread-1  ]: Using databricks connection "model.dbt_fivetran_1.4599_system_note"
07:21:47.553978 [debug] [Thread-1  ]: On model.dbt_fivetran_1.4599_system_note: /* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "node_id": "model.dbt_fivetran_1.4599_system_note"} */
create or replace view dbt.4599_system_note
  
  
  as
    select action,audit_file from 4599_system_note limit 10;

07:21:47.553978 [debug] [Thread-1  ]: Opening a new connection, currently in state init
07:21:50.091087 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "node_id": "model.dbt_fivetran_1.4599_system_note"} */
create or replace view dbt.4599_system_note
  
  
  as
    select action,audit_file from 4599_system_note limit 10;

07:21:50.091087 [debug] [Thread-1  ]: Databricks adapter: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: 4599_system_note; line 6 pos 34;
'GlobalLimit 10
+- 'LocalLimit 10
   +- 'Project ['action, 'audit_file]
      +- 'UnresolvedRelation [4599_system_note], [], false

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: 4599_system_note; line 6 pos 34;
'GlobalLimit 10
+- 'LocalLimit 10
   +- 'Project ['action, 'audit_file]
      +- 'UnresolvedRelation [4599_system_note], [], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
	... 16 more

07:21:50.091087 [debug] [Thread-1  ]: finished collecting timing info
07:21:50.091087 [debug] [Thread-1  ]: On model.dbt_fivetran_1.4599_system_note: ROLLBACK
07:21:50.091087 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
07:21:50.091087 [debug] [Thread-1  ]: On model.dbt_fivetran_1.4599_system_note: Close
07:21:51.403110 [debug] [Thread-1  ]: Runtime Error in model 4599_system_note (models\example\4599_system_note.sql)
  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: 4599_system_note; line 6 pos 34;
  'GlobalLimit 10
  +- 'LocalLimit 10
     +- 'Project ['action, 'audit_file]
        +- 'UnresolvedRelation [4599_system_note], [], false
  
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
  	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
  	at java.security.AccessController.doPrivileged(Native Method)
  	at javax.security.auth.Subject.doAs(Subject.java:422)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
  	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	at java.lang.Thread.run(Thread.java:748)
  Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: 4599_system_note; line 6 pos 34;
  'GlobalLimit 10
  +- 'LocalLimit 10
     +- 'Project ['action, 'audit_file]
        +- 'UnresolvedRelation [4599_system_note], [], false
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
  	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
  	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
  	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
  	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
  	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
  	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
  	... 16 more
  
07:21:51.403110 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0c76c9c-4ac9-469e-aebd-e15a1650f2fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F09226BBB0>]}
07:21:51.403110 [error] [Thread-1  ]: 1 of 1 ERROR creating view model dbt.4599_system_note........................... [[31mERROR[0m in 3.96s]
07:21:51.411097 [debug] [Thread-1  ]: Finished running node model.dbt_fivetran_1.4599_system_note
07:21:51.419101 [debug] [MainThread]: Acquiring new databricks connection "master"
07:21:51.419101 [debug] [MainThread]: On master: ROLLBACK
07:21:51.419101 [debug] [MainThread]: Opening a new connection, currently in state init
07:21:52.657465 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
07:21:52.657465 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
07:21:52.657465 [debug] [MainThread]: Spark adapter: NotImplemented: commit
07:21:52.657465 [debug] [MainThread]: On master: ROLLBACK
07:21:52.665465 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
07:21:52.665465 [debug] [MainThread]: On master: Close
07:21:53.856112 [info ] [MainThread]: 
07:21:53.864076 [info ] [MainThread]: Finished running 1 view model in 25.45s.
07:21:53.864076 [debug] [MainThread]: Connection 'master' was properly closed.
07:21:53.864076 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
07:21:53.864076 [debug] [MainThread]: Connection 'list_None_dbt' was properly closed.
07:21:53.872035 [debug] [MainThread]: Connection 'model.dbt_fivetran_1.4599_system_note' was properly closed.
07:21:53.904083 [info ] [MainThread]: 
07:21:53.904083 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
07:21:53.912041 [info ] [MainThread]: 
07:21:53.920028 [error] [MainThread]: [33mRuntime Error in model 4599_system_note (models\example\4599_system_note.sql)[0m
07:21:53.920028 [error] [MainThread]:   org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: 4599_system_note; line 6 pos 34;
07:21:53.928028 [error] [MainThread]:   'GlobalLimit 10
07:21:53.928028 [error] [MainThread]:   +- 'LocalLimit 10
07:21:53.936037 [error] [MainThread]:      +- 'Project ['action, 'audit_file]
07:21:53.936037 [error] [MainThread]:         +- 'UnresolvedRelation [4599_system_note], [], false
07:21:53.936037 [error] [MainThread]:   
07:21:53.936037 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
07:21:53.936037 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
07:21:53.944023 [error] [MainThread]:   	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
07:21:53.944023 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
07:21:53.944023 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
07:21:53.944023 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
07:21:53.944023 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
07:21:53.952023 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
07:21:53.952023 [error] [MainThread]:   	at java.security.AccessController.doPrivileged(Native Method)
07:21:53.952023 [error] [MainThread]:   	at javax.security.auth.Subject.doAs(Subject.java:422)
07:21:53.952023 [error] [MainThread]:   	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
07:21:53.960024 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
07:21:53.960024 [error] [MainThread]:   	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
07:21:53.960024 [error] [MainThread]:   	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
07:21:53.968035 [error] [MainThread]:   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
07:21:53.968035 [error] [MainThread]:   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
07:21:53.968035 [error] [MainThread]:   	at java.lang.Thread.run(Thread.java:748)
07:21:53.968035 [error] [MainThread]:   Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: 4599_system_note; line 6 pos 34;
07:21:53.968035 [error] [MainThread]:   'GlobalLimit 10
07:21:53.976022 [error] [MainThread]:   +- 'LocalLimit 10
07:21:53.976022 [error] [MainThread]:      +- 'Project ['action, 'audit_file]
07:21:53.976022 [error] [MainThread]:         +- 'UnresolvedRelation [4599_system_note], [], false
07:21:53.976022 [error] [MainThread]:   
07:21:53.976022 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
07:21:53.984025 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
07:21:53.984025 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
07:21:53.984025 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
07:21:53.984025 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
07:21:53.992025 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
07:21:53.992025 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
07:21:53.992025 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
07:21:53.992025 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
07:21:53.992025 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
07:21:53.992025 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
07:21:54.000026 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
07:21:54.000026 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
07:21:54.000026 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
07:21:54.000026 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
07:21:54.000026 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
07:21:54.008023 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
07:21:54.008023 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
07:21:54.008023 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
07:21:54.008023 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
07:21:54.008023 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
07:21:54.016027 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
07:21:54.016027 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
07:21:54.016027 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
07:21:54.016027 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
07:21:54.016027 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
07:21:54.024026 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
07:21:54.024026 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
07:21:54.024026 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
07:21:54.024026 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
07:21:54.024026 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
07:21:54.024026 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
07:21:54.032025 [error] [MainThread]:   	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
07:21:54.032025 [error] [MainThread]:   	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
07:21:54.032025 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
07:21:54.032025 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
07:21:54.032025 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
07:21:54.040026 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
07:21:54.040026 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
07:21:54.040026 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
07:21:54.040026 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
07:21:54.040026 [error] [MainThread]:   	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
07:21:54.048025 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
07:21:54.048025 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
07:21:54.048025 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
07:21:54.048025 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
07:21:54.048025 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
07:21:54.048025 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
07:21:54.056024 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
07:21:54.056024 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
07:21:54.056024 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
07:21:54.056024 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
07:21:54.064026 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
07:21:54.064026 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
07:21:54.064026 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
07:21:54.072024 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
07:21:54.072024 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
07:21:54.072024 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
07:21:54.072024 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
07:21:54.072024 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
07:21:54.072024 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
07:21:54.080023 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
07:21:54.080023 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
07:21:54.080023 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
07:21:54.080023 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
07:21:54.088026 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
07:21:54.088026 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
07:21:54.088026 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
07:21:54.088026 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
07:21:54.088026 [error] [MainThread]:   	... 16 more
07:21:54.096026 [error] [MainThread]:   
07:21:54.096026 [info ] [MainThread]: 
07:21:54.096026 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
07:21:54.096026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F092248FA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F092151610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F092151190>]}


============================== 2022-06-27 14:04:35.346503 | afd38396-35f8-4244-8900-c11ce5934860 ==============================
14:04:35.346503 [info ] [MainThread]: Running with dbt=1.0.5
14:04:35.354503 [debug] [MainThread]: running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=None, defer=None, event_buffer_size=None, exclude=None, fail_fast=None, full_refresh=False, log_cache_events=False, log_format=None, partial_parse=None, printer_width=None, profile=None, profiles_dir='C:\\Users\\DELL\\.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=['example.4599_system_note'], selector_name=None, send_anonymous_usage_stats=None, single_threaded=False, state=None, static_parser=None, target=None, threads=None, use_colors=None, use_experimental_parser=None, vars='{}', version_check=None, warn_error=None, which='run', write_json=None)
14:04:35.354503 [debug] [MainThread]: Tracking: tracking
14:04:35.362497 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A98A151DC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A98A151E50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A98A151E80>]}
14:04:35.610490 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
14:04:35.610490 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
14:04:35.626489 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'afd38396-35f8-4244-8900-c11ce5934860', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A98A2675E0>]}
14:04:35.650486 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'afd38396-35f8-4244-8900-c11ce5934860', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A98A138940>]}
14:04:35.650486 [info ] [MainThread]: Found 4 models, 6 tests, 0 snapshots, 0 analyses, 218 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
14:04:35.650486 [info ] [MainThread]: 
14:04:35.650486 [debug] [MainThread]: Acquiring new databricks connection "master"
14:04:35.658486 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
14:04:35.658486 [debug] [ThreadPool]: Using databricks connection "list_schemas"
14:04:35.658486 [debug] [ThreadPool]: On list_schemas: GetSchemas(database=None, schema=None)
14:04:35.658486 [debug] [ThreadPool]: Opening a new connection, currently in state init
14:19:05.267540 [debug] [ThreadPool]: Databricks adapter: Error while running:
GetSchemas(database=None, schema=None)
14:19:05.267540 [debug] [ThreadPool]: Databricks adapter: Database Error
  failed to connect
14:19:05.267540 [debug] [MainThread]: Connection 'master' was properly closed.
14:19:05.267540 [debug] [MainThread]: Connection 'list_schemas' was properly closed.


============================== 2022-06-27 14:54:23.261295 | ea2f51d8-13d6-4785-9686-f45f6f008f1a ==============================
14:54:23.261295 [info ] [MainThread]: Running with dbt=1.0.5
14:54:23.261295 [debug] [MainThread]: running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=None, defer=None, event_buffer_size=None, exclude=None, fail_fast=None, full_refresh=False, log_cache_events=False, log_format=None, partial_parse=None, printer_width=None, profile=None, profiles_dir='C:\\Users\\DELL\\.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=['example.4599_system_note'], selector_name=None, send_anonymous_usage_stats=None, single_threaded=False, state=None, static_parser=None, target=None, threads=None, use_colors=None, use_experimental_parser=None, vars='{}', version_check=None, warn_error=None, which='run', write_json=None)
14:54:23.269287 [debug] [MainThread]: Tracking: tracking
14:54:23.269287 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001976BCB3E20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001976BCB3EB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001976BCB3EE0>]}
14:54:23.477310 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
14:54:23.477310 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
14:54:23.501288 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ea2f51d8-13d6-4785-9686-f45f6f008f1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001976BDC85E0>]}
14:54:23.533294 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ea2f51d8-13d6-4785-9686-f45f6f008f1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001976BC98940>]}
14:54:23.533294 [info ] [MainThread]: Found 4 models, 6 tests, 0 snapshots, 0 analyses, 218 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
14:54:23.541293 [info ] [MainThread]: 
14:54:23.549292 [debug] [MainThread]: Acquiring new databricks connection "master"
14:54:23.557294 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
14:54:23.557294 [debug] [ThreadPool]: Using databricks connection "list_schemas"
14:54:23.557294 [debug] [ThreadPool]: On list_schemas: GetSchemas(database=None, schema=None)
14:54:23.557294 [debug] [ThreadPool]: Opening a new connection, currently in state init
14:54:28.622032 [debug] [ThreadPool]: SQL status: OK in 5.06 seconds
14:54:28.678029 [debug] [ThreadPool]: On list_schemas: Close
14:54:38.639968 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_dbt"
14:54:38.655978 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
14:54:38.655978 [debug] [ThreadPool]: Using databricks connection "list_None_dbt"
14:54:38.655978 [debug] [ThreadPool]: On list_None_dbt: /* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "connection_name": "list_None_dbt"} */
show table extended in dbt like '*'
  
14:54:38.655978 [debug] [ThreadPool]: Opening a new connection, currently in state init
14:54:43.557246 [debug] [ThreadPool]: SQL status: OK in 4.9 seconds
14:54:43.565247 [debug] [ThreadPool]: On list_None_dbt: ROLLBACK
14:54:43.565247 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
14:54:43.565247 [debug] [ThreadPool]: On list_None_dbt: Close
14:54:44.621253 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
14:54:44.621253 [debug] [MainThread]: Spark adapter: NotImplemented: commit
14:54:44.621253 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
14:54:44.621253 [info ] [MainThread]: 
14:54:44.645253 [debug] [Thread-1  ]: Began running node model.dbt_fivetran_1.4599_system_note
14:54:44.653257 [info ] [Thread-1  ]: 1 of 1 START view model dbt.4599_system_note.................................... [RUN]
14:54:44.653257 [debug] [Thread-1  ]: Acquiring new databricks connection "model.dbt_fivetran_1.4599_system_note"
14:54:44.653257 [debug] [Thread-1  ]: Began compiling node model.dbt_fivetran_1.4599_system_note
14:54:44.653257 [debug] [Thread-1  ]: Compiling model.dbt_fivetran_1.4599_system_note
14:54:44.661253 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_fivetran_1.4599_system_note"
14:54:44.669261 [debug] [Thread-1  ]: finished collecting timing info
14:54:44.669261 [debug] [Thread-1  ]: Began executing node model.dbt_fivetran_1.4599_system_note
14:54:44.757248 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_fivetran_1.4599_system_note"
14:54:44.757248 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
14:54:44.757248 [debug] [Thread-1  ]: Using databricks connection "model.dbt_fivetran_1.4599_system_note"
14:54:44.757248 [debug] [Thread-1  ]: On model.dbt_fivetran_1.4599_system_note: /* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "node_id": "model.dbt_fivetran_1.4599_system_note"} */
create or replace view dbt.4599_system_note
  
  
  as
    select action,audit_file from 4599_system_note limit 10;

14:54:44.757248 [debug] [Thread-1  ]: Opening a new connection, currently in state init
14:54:47.573960 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "node_id": "model.dbt_fivetran_1.4599_system_note"} */
create or replace view dbt.4599_system_note
  
  
  as
    select action,audit_file from 4599_system_note limit 10;

14:54:47.573960 [debug] [Thread-1  ]: Databricks adapter: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: 4599_system_note; line 6 pos 34;
'GlobalLimit 10
+- 'LocalLimit 10
   +- 'Project ['action, 'audit_file]
      +- 'UnresolvedRelation [4599_system_note], [], false

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: 4599_system_note; line 6 pos 34;
'GlobalLimit 10
+- 'LocalLimit 10
   +- 'Project ['action, 'audit_file]
      +- 'UnresolvedRelation [4599_system_note], [], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
	... 16 more

14:54:47.573960 [debug] [Thread-1  ]: finished collecting timing info
14:54:47.573960 [debug] [Thread-1  ]: On model.dbt_fivetran_1.4599_system_note: ROLLBACK
14:54:47.573960 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
14:54:47.573960 [debug] [Thread-1  ]: On model.dbt_fivetran_1.4599_system_note: Close
14:54:48.781961 [debug] [Thread-1  ]: Runtime Error in model 4599_system_note (models\example\4599_system_note.sql)
  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: 4599_system_note; line 6 pos 34;
  'GlobalLimit 10
  +- 'LocalLimit 10
     +- 'Project ['action, 'audit_file]
        +- 'UnresolvedRelation [4599_system_note], [], false
  
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
  	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
  	at java.security.AccessController.doPrivileged(Native Method)
  	at javax.security.auth.Subject.doAs(Subject.java:422)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
  	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	at java.lang.Thread.run(Thread.java:748)
  Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: 4599_system_note; line 6 pos 34;
  'GlobalLimit 10
  +- 'LocalLimit 10
     +- 'Project ['action, 'audit_file]
        +- 'UnresolvedRelation [4599_system_note], [], false
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
  	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
  	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
  	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
  	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
  	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
  	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
  	... 16 more
  
14:54:48.781961 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ea2f51d8-13d6-4785-9686-f45f6f008f1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001976BF0A040>]}
14:54:48.781961 [error] [Thread-1  ]: 1 of 1 ERROR creating view model dbt.4599_system_note........................... [[31mERROR[0m in 4.13s]
14:54:48.781961 [debug] [Thread-1  ]: Finished running node model.dbt_fivetran_1.4599_system_note
14:54:48.781961 [debug] [MainThread]: Acquiring new databricks connection "master"
14:54:48.781961 [debug] [MainThread]: On master: ROLLBACK
14:54:48.781961 [debug] [MainThread]: Opening a new connection, currently in state init
14:54:49.890529 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
14:54:49.890529 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
14:54:49.890529 [debug] [MainThread]: Spark adapter: NotImplemented: commit
14:54:49.890529 [debug] [MainThread]: On master: ROLLBACK
14:54:49.890529 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
14:54:49.890529 [debug] [MainThread]: On master: Close
14:54:50.954489 [info ] [MainThread]: 
14:54:50.954489 [info ] [MainThread]: Finished running 1 view model in 27.41s.
14:54:50.954489 [debug] [MainThread]: Connection 'master' was properly closed.
14:54:50.954489 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
14:54:50.954489 [debug] [MainThread]: Connection 'list_None_dbt' was properly closed.
14:54:50.954489 [debug] [MainThread]: Connection 'model.dbt_fivetran_1.4599_system_note' was properly closed.
14:54:50.970505 [info ] [MainThread]: 
14:54:50.970505 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
14:54:50.978490 [info ] [MainThread]: 
14:54:50.978490 [error] [MainThread]: [33mRuntime Error in model 4599_system_note (models\example\4599_system_note.sql)[0m
14:54:50.978490 [error] [MainThread]:   org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: 4599_system_note; line 6 pos 34;
14:54:50.978490 [error] [MainThread]:   'GlobalLimit 10
14:54:50.978490 [error] [MainThread]:   +- 'LocalLimit 10
14:54:50.978490 [error] [MainThread]:      +- 'Project ['action, 'audit_file]
14:54:50.986489 [error] [MainThread]:         +- 'UnresolvedRelation [4599_system_note], [], false
14:54:50.986489 [error] [MainThread]:   
14:54:50.986489 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
14:54:50.986489 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
14:54:50.986489 [error] [MainThread]:   	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
14:54:50.994490 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
14:54:50.994490 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
14:54:50.994490 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
14:54:51.002490 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
14:54:51.002490 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
14:54:51.002490 [error] [MainThread]:   	at java.security.AccessController.doPrivileged(Native Method)
14:54:51.002490 [error] [MainThread]:   	at javax.security.auth.Subject.doAs(Subject.java:422)
14:54:51.010489 [error] [MainThread]:   	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
14:54:51.010489 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
14:54:51.010489 [error] [MainThread]:   	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
14:54:51.018498 [error] [MainThread]:   	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
14:54:51.018498 [error] [MainThread]:   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
14:54:51.018498 [error] [MainThread]:   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
14:54:51.018498 [error] [MainThread]:   	at java.lang.Thread.run(Thread.java:748)
14:54:51.018498 [error] [MainThread]:   Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: 4599_system_note; line 6 pos 34;
14:54:51.018498 [error] [MainThread]:   'GlobalLimit 10
14:54:51.026490 [error] [MainThread]:   +- 'LocalLimit 10
14:54:51.026490 [error] [MainThread]:      +- 'Project ['action, 'audit_file]
14:54:51.026490 [error] [MainThread]:         +- 'UnresolvedRelation [4599_system_note], [], false
14:54:51.034489 [error] [MainThread]:   
14:54:51.034489 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
14:54:51.034489 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
14:54:51.034489 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
14:54:51.034489 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
14:54:51.042491 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
14:54:51.042491 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
14:54:51.042491 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
14:54:51.042491 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
14:54:51.042491 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
14:54:51.050491 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
14:54:51.050491 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
14:54:51.050491 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
14:54:51.050491 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
14:54:51.060340 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
14:54:51.060340 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
14:54:51.064772 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
14:54:51.064772 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
14:54:51.064772 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
14:54:51.064772 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
14:54:51.064772 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
14:54:51.072765 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
14:54:51.072765 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
14:54:51.072765 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
14:54:51.072765 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
14:54:51.072765 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
14:54:51.080781 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
14:54:51.080781 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
14:54:51.080781 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
14:54:51.080781 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
14:54:51.080781 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
14:54:51.088768 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
14:54:51.088768 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
14:54:51.088768 [error] [MainThread]:   	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
14:54:51.088768 [error] [MainThread]:   	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
14:54:51.088768 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
14:54:51.096766 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
14:54:51.096766 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
14:54:51.096766 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
14:54:51.096766 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
14:54:51.096766 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
14:54:51.104767 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
14:54:51.104767 [error] [MainThread]:   	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
14:54:51.104767 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
14:54:51.104767 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
14:54:51.112770 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
14:54:51.112770 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
14:54:51.112770 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
14:54:51.112770 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
14:54:51.112770 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
14:54:51.120769 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
14:54:51.120769 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
14:54:51.120769 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
14:54:51.128777 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
14:54:51.128777 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
14:54:51.128777 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
14:54:51.128777 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
14:54:51.136766 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
14:54:51.136766 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
14:54:51.136766 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
14:54:51.136766 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
14:54:51.136766 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
14:54:51.136766 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
14:54:51.144768 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
14:54:51.144768 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
14:54:51.144768 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
14:54:51.144768 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
14:54:51.152766 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
14:54:51.152766 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
14:54:51.152766 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
14:54:51.152766 [error] [MainThread]:   	... 16 more
14:54:51.160770 [error] [MainThread]:   
14:54:51.160770 [info ] [MainThread]: 
14:54:51.160770 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
14:54:51.160770 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001976BDC8C10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001976BDB38B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001976BDB30D0>]}


============================== 2022-06-27 17:31:31.887321 | 1b055664-4492-4fee-8b19-8df273d96bd5 ==============================
17:31:31.887321 [info ] [MainThread]: Running with dbt=1.0.5
17:31:31.887321 [debug] [MainThread]: running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=None, defer=None, event_buffer_size=None, exclude=None, fail_fast=None, full_refresh=False, log_cache_events=False, log_format=None, partial_parse=None, printer_width=None, profile=None, profiles_dir='C:\\Users\\DELL\\.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=['example.4599_system_note'], selector_name=None, send_anonymous_usage_stats=None, single_threaded=False, state=None, static_parser=None, target=None, threads=None, use_colors=None, use_experimental_parser=None, vars='{}', version_check=None, warn_error=None, which='run', write_json=None)
17:31:31.887321 [debug] [MainThread]: Tracking: tracking
17:31:31.887321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E2ACD31DC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E2ACD31E50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E2ACD31E80>]}
17:31:31.968412 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
17:31:31.973425 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '1b055664-4492-4fee-8b19-8df273d96bd5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E2ACD18B20>]}
17:31:32.108379 [debug] [MainThread]: Parsing macros\adapters.sql
17:31:32.143125 [debug] [MainThread]: Parsing macros\materializations\seed.sql
17:31:32.157310 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
17:31:32.173305 [debug] [MainThread]: Parsing macros\materializations\table.sql
17:31:32.198221 [debug] [MainThread]: Parsing macros\materializations\view.sql
17:31:32.198221 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
17:31:32.207323 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
17:31:32.215334 [debug] [MainThread]: Parsing macros\materializations\incremental\validate.sql
17:31:32.223382 [debug] [MainThread]: Parsing macros\adapters.sql
17:31:32.297292 [debug] [MainThread]: Parsing macros\materializations\seed.sql
17:31:32.313354 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
17:31:32.348021 [debug] [MainThread]: Parsing macros\materializations\table.sql
17:31:32.348021 [debug] [MainThread]: Parsing macros\materializations\view.sql
17:31:32.358244 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
17:31:32.367334 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
17:31:32.383350 [debug] [MainThread]: Parsing macros\materializations\incremental\validate.sql
17:31:32.391328 [debug] [MainThread]: Parsing macros\adapters\columns.sql
17:31:32.412959 [debug] [MainThread]: Parsing macros\adapters\freshness.sql
17:31:32.418011 [debug] [MainThread]: Parsing macros\adapters\indexes.sql
17:31:32.423023 [debug] [MainThread]: Parsing macros\adapters\metadata.sql
17:31:32.438432 [debug] [MainThread]: Parsing macros\adapters\persist_docs.sql
17:31:32.442944 [debug] [MainThread]: Parsing macros\adapters\relation.sql
17:31:32.465315 [debug] [MainThread]: Parsing macros\adapters\schema.sql
17:31:32.473326 [debug] [MainThread]: Parsing macros\etc\datetime.sql
17:31:32.488384 [debug] [MainThread]: Parsing macros\etc\statement.sql
17:31:32.505313 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
17:31:32.508368 [debug] [MainThread]: Parsing macros\generic_test_sql\not_null.sql
17:31:32.513381 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
17:31:32.513381 [debug] [MainThread]: Parsing macros\generic_test_sql\unique.sql
17:31:32.513381 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_alias.sql
17:31:32.518433 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_database.sql
17:31:32.518433 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_schema.sql
17:31:32.522947 [debug] [MainThread]: Parsing macros\materializations\configs.sql
17:31:32.527999 [debug] [MainThread]: Parsing macros\materializations\hooks.sql
17:31:32.538111 [debug] [MainThread]: Parsing macros\materializations\models\incremental\column_helpers.sql
17:31:32.538111 [debug] [MainThread]: Parsing macros\materializations\models\incremental\incremental.sql
17:31:32.563319 [debug] [MainThread]: Parsing macros\materializations\models\incremental\is_incremental.sql
17:31:32.563319 [debug] [MainThread]: Parsing macros\materializations\models\incremental\merge.sql
17:31:32.587317 [debug] [MainThread]: Parsing macros\materializations\models\incremental\on_schema_change.sql
17:31:32.619315 [debug] [MainThread]: Parsing macros\materializations\models\table\create_table_as.sql
17:31:32.619315 [debug] [MainThread]: Parsing macros\materializations\models\table\table.sql
17:31:32.638430 [debug] [MainThread]: Parsing macros\materializations\models\view\create_or_replace_view.sql
17:31:32.643456 [debug] [MainThread]: Parsing macros\materializations\models\view\create_view_as.sql
17:31:32.648059 [debug] [MainThread]: Parsing macros\materializations\models\view\helpers.sql
17:31:32.648059 [debug] [MainThread]: Parsing macros\materializations\models\view\view.sql
17:31:32.665327 [debug] [MainThread]: Parsing macros\materializations\seeds\helpers.sql
17:31:32.698034 [debug] [MainThread]: Parsing macros\materializations\seeds\seed.sql
17:31:32.707322 [debug] [MainThread]: Parsing macros\materializations\snapshots\helpers.sql
17:31:32.738404 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot.sql
17:31:32.757326 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot_merge.sql
17:31:32.757326 [debug] [MainThread]: Parsing macros\materializations\snapshots\strategies.sql
17:31:32.793329 [debug] [MainThread]: Parsing macros\materializations\tests\helpers.sql
17:31:32.801324 [debug] [MainThread]: Parsing macros\materializations\tests\test.sql
17:31:32.808398 [debug] [MainThread]: Parsing macros\materializations\tests\where_subquery.sql
17:31:32.808398 [debug] [MainThread]: Parsing tests\generic\builtin.sql
17:31:33.458151 [debug] [MainThread]: 1699: static parser successfully parsed example\100kusd.sql
17:31:33.477537 [debug] [MainThread]: 1699: static parser successfully parsed example\4599_system_note.sql
17:31:33.477537 [debug] [MainThread]: 1699: static parser successfully parsed example\my_first_dbt_model.sql
17:31:33.485541 [debug] [MainThread]: 1699: static parser successfully parsed example\my_second_dbt_model.sql
17:31:33.595326 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_fivetran.example

17:31:33.608283 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1b055664-4492-4fee-8b19-8df273d96bd5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E2ACC959A0>]}
17:31:33.618258 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1b055664-4492-4fee-8b19-8df273d96bd5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E2ACC958E0>]}
17:31:33.618258 [info ] [MainThread]: Found 4 models, 6 tests, 0 snapshots, 0 analyses, 218 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
17:31:33.628593 [info ] [MainThread]: 
17:31:33.628593 [debug] [MainThread]: Acquiring new databricks connection "master"
17:31:33.636031 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
17:31:33.636031 [debug] [ThreadPool]: Using databricks connection "list_schemas"
17:31:33.636031 [debug] [ThreadPool]: On list_schemas: GetSchemas(database=None, schema=None)
17:31:33.638629 [debug] [ThreadPool]: Opening a new connection, currently in state init
17:31:37.923568 [debug] [ThreadPool]: SQL status: OK in 4.28 seconds
17:31:37.957482 [debug] [ThreadPool]: On list_schemas: Close
17:31:39.059794 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_dbt"
17:31:39.078590 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
17:31:39.078590 [debug] [ThreadPool]: Using databricks connection "list_None_dbt"
17:31:39.078590 [debug] [ThreadPool]: On list_None_dbt: /* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "connection_name": "list_None_dbt"} */
show table extended in dbt like '*'
  
17:31:39.078590 [debug] [ThreadPool]: Opening a new connection, currently in state init
17:31:41.483525 [debug] [ThreadPool]: SQL status: OK in 2.4 seconds
17:31:41.488687 [debug] [ThreadPool]: On list_None_dbt: ROLLBACK
17:31:41.488687 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
17:31:41.488687 [debug] [ThreadPool]: On list_None_dbt: Close
17:31:42.527670 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
17:31:42.527670 [debug] [MainThread]: Spark adapter: NotImplemented: commit
17:31:42.527670 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
17:31:42.535651 [info ] [MainThread]: 
17:31:42.548421 [debug] [Thread-1  ]: Began running node model.dbt_fivetran_1.4599_system_note
17:31:42.548421 [info ] [Thread-1  ]: 1 of 1 START view model dbt.4599_system_note.................................... [RUN]
17:31:42.548421 [debug] [Thread-1  ]: Acquiring new databricks connection "model.dbt_fivetran_1.4599_system_note"
17:31:42.548421 [debug] [Thread-1  ]: Began compiling node model.dbt_fivetran_1.4599_system_note
17:31:42.548421 [debug] [Thread-1  ]: Compiling model.dbt_fivetran_1.4599_system_note
17:31:42.553435 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_fivetran_1.4599_system_note"
17:31:42.557036 [debug] [Thread-1  ]: finished collecting timing info
17:31:42.557036 [debug] [Thread-1  ]: Began executing node model.dbt_fivetran_1.4599_system_note
17:31:42.633558 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_fivetran_1.4599_system_note"
17:31:42.638671 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
17:31:42.638671 [debug] [Thread-1  ]: Using databricks connection "model.dbt_fivetran_1.4599_system_note"
17:31:42.638671 [debug] [Thread-1  ]: On model.dbt_fivetran_1.4599_system_note: /* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "node_id": "model.dbt_fivetran_1.4599_system_note"} */
create or replace view dbt.4599_system_note
  
  
  as
    select action,audit_file from netsuite_suiteanalytics.4599_system_note limit 10;

17:31:42.638671 [debug] [Thread-1  ]: Opening a new connection, currently in state init
17:31:46.493784 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "node_id": "model.dbt_fivetran_1.4599_system_note"} */
create or replace view dbt.4599_system_note
  
  
  as
    select action,audit_file from netsuite_suiteanalytics.4599_system_note limit 10;

17:31:46.493784 [debug] [Thread-1  ]: Databricks adapter: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: netsuite_suiteanalytics.4599_system_note; line 6 pos 34;
'GlobalLimit 10
+- 'LocalLimit 10
   +- 'Project ['action, 'audit_file]
      +- 'UnresolvedRelation [netsuite_suiteanalytics, 4599_system_note], [], false

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: netsuite_suiteanalytics.4599_system_note; line 6 pos 34;
'GlobalLimit 10
+- 'LocalLimit 10
   +- 'Project ['action, 'audit_file]
      +- 'UnresolvedRelation [netsuite_suiteanalytics, 4599_system_note], [], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
	... 16 more

17:31:46.501747 [debug] [Thread-1  ]: finished collecting timing info
17:31:46.501747 [debug] [Thread-1  ]: On model.dbt_fivetran_1.4599_system_note: ROLLBACK
17:31:46.501747 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
17:31:46.501747 [debug] [Thread-1  ]: On model.dbt_fivetran_1.4599_system_note: Close
17:31:47.538792 [debug] [Thread-1  ]: Runtime Error in model 4599_system_note (models\example\4599_system_note.sql)
  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: netsuite_suiteanalytics.4599_system_note; line 6 pos 34;
  'GlobalLimit 10
  +- 'LocalLimit 10
     +- 'Project ['action, 'audit_file]
        +- 'UnresolvedRelation [netsuite_suiteanalytics, 4599_system_note], [], false
  
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
  	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
  	at java.security.AccessController.doPrivileged(Native Method)
  	at javax.security.auth.Subject.doAs(Subject.java:422)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
  	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	at java.lang.Thread.run(Thread.java:748)
  Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: netsuite_suiteanalytics.4599_system_note; line 6 pos 34;
  'GlobalLimit 10
  +- 'LocalLimit 10
     +- 'Project ['action, 'audit_file]
        +- 'UnresolvedRelation [netsuite_suiteanalytics, 4599_system_note], [], false
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
  	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
  	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
  	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
  	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
  	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
  	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
  	... 16 more
  
17:31:47.547929 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1b055664-4492-4fee-8b19-8df273d96bd5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E2ACE2EC70>]}
17:31:47.547929 [error] [Thread-1  ]: 1 of 1 ERROR creating view model dbt.4599_system_note........................... [[31mERROR[0m in 5.00s]
17:31:47.547929 [debug] [Thread-1  ]: Finished running node model.dbt_fivetran_1.4599_system_note
17:31:47.555905 [debug] [MainThread]: Acquiring new databricks connection "master"
17:31:47.555905 [debug] [MainThread]: On master: ROLLBACK
17:31:47.555905 [debug] [MainThread]: Opening a new connection, currently in state init
17:31:48.678952 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
17:31:48.688922 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
17:31:48.688922 [debug] [MainThread]: Spark adapter: NotImplemented: commit
17:31:48.688922 [debug] [MainThread]: On master: ROLLBACK
17:31:48.688922 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
17:31:48.688922 [debug] [MainThread]: On master: Close
17:31:49.719479 [info ] [MainThread]: 
17:31:49.719479 [info ] [MainThread]: Finished running 1 view model in 16.09s.
17:31:49.719479 [debug] [MainThread]: Connection 'master' was properly closed.
17:31:49.727484 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
17:31:49.727484 [debug] [MainThread]: Connection 'list_None_dbt' was properly closed.
17:31:49.727484 [debug] [MainThread]: Connection 'model.dbt_fivetran_1.4599_system_note' was properly closed.
17:31:49.763957 [info ] [MainThread]: 
17:31:49.768666 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
17:31:49.768666 [info ] [MainThread]: 
17:31:49.784006 [error] [MainThread]: [33mRuntime Error in model 4599_system_note (models\example\4599_system_note.sql)[0m
17:31:49.788655 [error] [MainThread]:   org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: netsuite_suiteanalytics.4599_system_note; line 6 pos 34;
17:31:49.788655 [error] [MainThread]:   'GlobalLimit 10
17:31:49.803620 [error] [MainThread]:   +- 'LocalLimit 10
17:31:49.803620 [error] [MainThread]:      +- 'Project ['action, 'audit_file]
17:31:49.808880 [error] [MainThread]:         +- 'UnresolvedRelation [netsuite_suiteanalytics, 4599_system_note], [], false
17:31:49.808880 [error] [MainThread]:   
17:31:49.808880 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
17:31:49.808880 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
17:31:49.818790 [error] [MainThread]:   	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
17:31:49.823814 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
17:31:49.823814 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
17:31:49.828457 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
17:31:49.828457 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
17:31:49.833480 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
17:31:49.833480 [error] [MainThread]:   	at java.security.AccessController.doPrivileged(Native Method)
17:31:49.838641 [error] [MainThread]:   	at javax.security.auth.Subject.doAs(Subject.java:422)
17:31:49.843660 [error] [MainThread]:   	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
17:31:49.843660 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
17:31:49.843660 [error] [MainThread]:   	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
17:31:49.848763 [error] [MainThread]:   	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
17:31:49.848763 [error] [MainThread]:   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
17:31:49.853776 [error] [MainThread]:   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
17:31:49.853776 [error] [MainThread]:   	at java.lang.Thread.run(Thread.java:748)
17:31:49.853776 [error] [MainThread]:   Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: netsuite_suiteanalytics.4599_system_note; line 6 pos 34;
17:31:49.853776 [error] [MainThread]:   'GlobalLimit 10
17:31:49.858885 [error] [MainThread]:   +- 'LocalLimit 10
17:31:49.858885 [error] [MainThread]:      +- 'Project ['action, 'audit_file]
17:31:49.858885 [error] [MainThread]:         +- 'UnresolvedRelation [netsuite_suiteanalytics, 4599_system_note], [], false
17:31:49.863401 [error] [MainThread]:   
17:31:49.863401 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
17:31:49.863401 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
17:31:49.863401 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
17:31:49.868465 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
17:31:49.868465 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
17:31:49.873486 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
17:31:49.875541 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
17:31:49.875541 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
17:31:49.878657 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
17:31:49.878657 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
17:31:49.878657 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
17:31:49.878657 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
17:31:49.883672 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
17:31:49.883672 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
17:31:49.883672 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
17:31:49.883672 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
17:31:49.888764 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
17:31:49.888764 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
17:31:49.888764 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
17:31:49.888764 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
17:31:49.893777 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
17:31:49.893777 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
17:31:49.893777 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
17:31:49.898949 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
17:31:49.903465 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
17:31:49.903465 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
17:31:49.903465 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
17:31:49.903465 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
17:31:49.908557 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
17:31:49.908557 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
17:31:49.908557 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
17:31:49.908557 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
17:31:49.913566 [error] [MainThread]:   	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
17:31:49.913566 [error] [MainThread]:   	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
17:31:49.913566 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
17:31:49.913566 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
17:31:49.918667 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
17:31:49.918667 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
17:31:49.918667 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
17:31:49.923682 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
17:31:49.923682 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
17:31:49.928818 [error] [MainThread]:   	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
17:31:49.928818 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
17:31:49.928818 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
17:31:49.933834 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
17:31:49.933834 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
17:31:49.938402 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
17:31:49.938402 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
17:31:49.938402 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
17:31:49.938402 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
17:31:49.943417 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
17:31:49.948552 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
17:31:49.948552 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
17:31:49.948552 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
17:31:49.953570 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
17:31:49.953570 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
17:31:49.953570 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
17:31:49.953570 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
17:31:49.958676 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
17:31:49.958676 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
17:31:49.958676 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
17:31:49.963689 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
17:31:49.963689 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
17:31:49.963689 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
17:31:49.968771 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
17:31:49.968771 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
17:31:49.968771 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
17:31:49.973783 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
17:31:49.975825 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
17:31:49.978404 [error] [MainThread]:   	... 16 more
17:31:49.978404 [error] [MainThread]:   
17:31:49.983429 [info ] [MainThread]: 
17:31:49.983429 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
17:31:49.983429 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E2ACDBA8E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E2ACDB7520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E2ACE07A30>]}


============================== 2022-06-27 17:35:58.418618 | 779f3bd9-1e92-4d0b-a20d-704360cd7056 ==============================
17:35:58.418618 [info ] [MainThread]: Running with dbt=1.0.5
17:35:58.418618 [debug] [MainThread]: running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=None, defer=None, event_buffer_size=None, exclude=None, fail_fast=None, full_refresh=False, log_cache_events=False, log_format=None, partial_parse=None, printer_width=None, profile=None, profiles_dir='C:\\Users\\DELL\\.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=['example.4599_system_note'], selector_name=None, send_anonymous_usage_stats=None, single_threaded=False, state=None, static_parser=None, target=None, threads=None, use_colors=None, use_experimental_parser=None, vars='{}', version_check=None, warn_error=None, which='run', write_json=None)
17:35:58.418618 [debug] [MainThread]: Tracking: tracking
17:35:58.418618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000287C95F4CA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000287C95F4D30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000287C95F4D60>]}
17:35:58.488883 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
17:35:58.488883 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '779f3bd9-1e92-4d0b-a20d-704360cd7056', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000287C959CB80>]}
17:35:58.598764 [debug] [MainThread]: Parsing macros\adapters.sql
17:35:58.633545 [debug] [MainThread]: Parsing macros\materializations\seed.sql
17:35:58.643646 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
17:35:58.663820 [debug] [MainThread]: Parsing macros\materializations\table.sql
17:35:58.668896 [debug] [MainThread]: Parsing macros\materializations\view.sql
17:35:58.668896 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
17:35:58.678982 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
17:35:58.688548 [debug] [MainThread]: Parsing macros\materializations\incremental\validate.sql
17:35:58.693563 [debug] [MainThread]: Parsing macros\adapters.sql
17:35:58.777818 [debug] [MainThread]: Parsing macros\materializations\seed.sql
17:35:58.801810 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
17:35:58.838996 [debug] [MainThread]: Parsing macros\materializations\table.sql
17:35:58.838996 [debug] [MainThread]: Parsing macros\materializations\view.sql
17:35:58.847812 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
17:35:58.855857 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
17:35:58.863924 [debug] [MainThread]: Parsing macros\materializations\incremental\validate.sql
17:35:58.868981 [debug] [MainThread]: Parsing macros\adapters\columns.sql
17:35:58.888866 [debug] [MainThread]: Parsing macros\adapters\freshness.sql
17:35:58.893877 [debug] [MainThread]: Parsing macros\adapters\indexes.sql
17:35:58.898934 [debug] [MainThread]: Parsing macros\adapters\metadata.sql
17:35:58.908525 [debug] [MainThread]: Parsing macros\adapters\persist_docs.sql
17:35:58.917831 [debug] [MainThread]: Parsing macros\adapters\relation.sql
17:35:58.938737 [debug] [MainThread]: Parsing macros\adapters\schema.sql
17:35:58.938737 [debug] [MainThread]: Parsing macros\etc\datetime.sql
17:35:58.968720 [debug] [MainThread]: Parsing macros\etc\statement.sql
17:35:58.973740 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
17:35:58.978830 [debug] [MainThread]: Parsing macros\generic_test_sql\not_null.sql
17:35:58.978830 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
17:35:58.978830 [debug] [MainThread]: Parsing macros\generic_test_sql\unique.sql
17:35:58.978830 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_alias.sql
17:35:58.989022 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_database.sql
17:35:58.989022 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_schema.sql
17:35:58.994049 [debug] [MainThread]: Parsing macros\materializations\configs.sql
17:35:58.998630 [debug] [MainThread]: Parsing macros\materializations\hooks.sql
17:35:59.008732 [debug] [MainThread]: Parsing macros\materializations\models\incremental\column_helpers.sql
17:35:59.017826 [debug] [MainThread]: Parsing macros\materializations\models\incremental\incremental.sql
17:35:59.037817 [debug] [MainThread]: Parsing macros\materializations\models\incremental\is_incremental.sql
17:35:59.037817 [debug] [MainThread]: Parsing macros\materializations\models\incremental\merge.sql
17:35:59.063685 [debug] [MainThread]: Parsing macros\materializations\models\incremental\on_schema_change.sql
17:35:59.093823 [debug] [MainThread]: Parsing macros\materializations\models\table\create_table_as.sql
17:35:59.098936 [debug] [MainThread]: Parsing macros\materializations\models\table\table.sql
17:35:59.113543 [debug] [MainThread]: Parsing macros\materializations\models\view\create_or_replace_view.sql
17:35:59.118632 [debug] [MainThread]: Parsing macros\materializations\models\view\create_view_as.sql
17:35:59.118632 [debug] [MainThread]: Parsing macros\materializations\models\view\helpers.sql
17:35:59.118632 [debug] [MainThread]: Parsing macros\materializations\models\view\view.sql
17:35:59.135831 [debug] [MainThread]: Parsing macros\materializations\seeds\helpers.sql
17:35:59.163580 [debug] [MainThread]: Parsing macros\materializations\seeds\seed.sql
17:35:59.173619 [debug] [MainThread]: Parsing macros\materializations\snapshots\helpers.sql
17:35:59.188682 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot.sql
17:35:59.205796 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot_merge.sql
17:35:59.213819 [debug] [MainThread]: Parsing macros\materializations\snapshots\strategies.sql
17:35:59.237820 [debug] [MainThread]: Parsing macros\materializations\tests\helpers.sql
17:35:59.237820 [debug] [MainThread]: Parsing macros\materializations\tests\test.sql
17:35:59.245820 [debug] [MainThread]: Parsing macros\materializations\tests\where_subquery.sql
17:35:59.253805 [debug] [MainThread]: Parsing tests\generic\builtin.sql
17:35:59.828940 [debug] [MainThread]: 1699: static parser successfully parsed example\100kusd.sql
17:35:59.855765 [debug] [MainThread]: 1699: static parser successfully parsed example\4599_system_note.sql
17:35:59.858863 [debug] [MainThread]: 1699: static parser successfully parsed example\my_first_dbt_model.sql
17:35:59.863877 [debug] [MainThread]: 1699: static parser successfully parsed example\my_second_dbt_model.sql
17:35:59.998482 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '779f3bd9-1e92-4d0b-a20d-704360cd7056', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000287C9566760>]}
17:36:00.018631 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '779f3bd9-1e92-4d0b-a20d-704360cd7056', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000287C958A730>]}
17:36:00.019105 [info ] [MainThread]: Found 4 models, 6 tests, 0 snapshots, 0 analyses, 218 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
17:36:00.019105 [info ] [MainThread]: 
17:36:00.024126 [debug] [MainThread]: Acquiring new databricks connection "master"
17:36:00.028722 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
17:36:00.028722 [debug] [ThreadPool]: Using databricks connection "list_schemas"
17:36:00.028722 [debug] [ThreadPool]: On list_schemas: GetSchemas(database=None, schema=None)
17:36:00.028722 [debug] [ThreadPool]: Opening a new connection, currently in state init
17:36:04.138269 [debug] [ThreadPool]: SQL status: OK in 4.11 seconds
17:36:04.138269 [debug] [ThreadPool]: On list_schemas: Close
17:36:05.198003 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_dbt"
17:36:05.255370 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
17:36:05.255370 [debug] [ThreadPool]: Using databricks connection "list_None_dbt"
17:36:05.255370 [debug] [ThreadPool]: On list_None_dbt: /* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "connection_name": "list_None_dbt"} */
show table extended in dbt like '*'
  
17:36:05.255370 [debug] [ThreadPool]: Opening a new connection, currently in state init
17:36:07.628640 [debug] [ThreadPool]: SQL status: OK in 2.37 seconds
17:36:07.643620 [debug] [ThreadPool]: On list_None_dbt: ROLLBACK
17:36:07.643620 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
17:36:07.643620 [debug] [ThreadPool]: On list_None_dbt: Close
17:36:08.708555 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
17:36:08.708555 [debug] [MainThread]: Spark adapter: NotImplemented: commit
17:36:08.708555 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
17:36:08.708555 [info ] [MainThread]: 
17:36:08.738689 [debug] [Thread-1  ]: Began running node model.dbt_fivetran_1.4599_system_note
17:36:08.738689 [info ] [Thread-1  ]: 1 of 1 START view model dbt.4599_system_note.................................... [RUN]
17:36:08.743723 [debug] [Thread-1  ]: Acquiring new databricks connection "model.dbt_fivetran_1.4599_system_note"
17:36:08.743723 [debug] [Thread-1  ]: Began compiling node model.dbt_fivetran_1.4599_system_note
17:36:08.748763 [debug] [Thread-1  ]: Compiling model.dbt_fivetran_1.4599_system_note
17:36:08.753474 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_fivetran_1.4599_system_note"
17:36:08.758700 [debug] [Thread-1  ]: finished collecting timing info
17:36:08.763739 [debug] [Thread-1  ]: Began executing node model.dbt_fivetran_1.4599_system_note
17:36:08.838592 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_fivetran_1.4599_system_note"
17:36:08.843612 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
17:36:08.843612 [debug] [Thread-1  ]: Using databricks connection "model.dbt_fivetran_1.4599_system_note"
17:36:08.843612 [debug] [Thread-1  ]: On model.dbt_fivetran_1.4599_system_note: /* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "node_id": "model.dbt_fivetran_1.4599_system_note"} */
create or replace view dbt.4599_system_note
  
  
  as
    select action,audit_file from netsuite_suiteanalytics.4599_system_note limit 10;

17:36:08.843612 [debug] [Thread-1  ]: Opening a new connection, currently in state init
17:36:26.108515 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.5", "profile_name": "dbt_fivetran_1", "target_name": "dev", "node_id": "model.dbt_fivetran_1.4599_system_note"} */
create or replace view dbt.4599_system_note
  
  
  as
    select action,audit_file from netsuite_suiteanalytics.4599_system_note limit 10;

17:36:26.108515 [debug] [Thread-1  ]: Databricks adapter: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: netsuite_suiteanalytics.4599_system_note; line 6 pos 34;
'GlobalLimit 10
+- 'LocalLimit 10
   +- 'Project ['action, 'audit_file]
      +- 'UnresolvedRelation [netsuite_suiteanalytics, 4599_system_note], [], false

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: netsuite_suiteanalytics.4599_system_note; line 6 pos 34;
'GlobalLimit 10
+- 'LocalLimit 10
   +- 'Project ['action, 'audit_file]
      +- 'UnresolvedRelation [netsuite_suiteanalytics, 4599_system_note], [], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
	... 16 more

17:36:26.108515 [debug] [Thread-1  ]: finished collecting timing info
17:36:26.108515 [debug] [Thread-1  ]: On model.dbt_fivetran_1.4599_system_note: ROLLBACK
17:36:26.108515 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
17:36:26.108515 [debug] [Thread-1  ]: On model.dbt_fivetran_1.4599_system_note: Close
17:36:27.188301 [debug] [Thread-1  ]: Runtime Error in model 4599_system_note (models\example\4599_system_note.sql)
  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: netsuite_suiteanalytics.4599_system_note; line 6 pos 34;
  'GlobalLimit 10
  +- 'LocalLimit 10
     +- 'Project ['action, 'audit_file]
        +- 'UnresolvedRelation [netsuite_suiteanalytics, 4599_system_note], [], false
  
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
  	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
  	at java.security.AccessController.doPrivileged(Native Method)
  	at javax.security.auth.Subject.doAs(Subject.java:422)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
  	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	at java.lang.Thread.run(Thread.java:748)
  Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: netsuite_suiteanalytics.4599_system_note; line 6 pos 34;
  'GlobalLimit 10
  +- 'LocalLimit 10
     +- 'Project ['action, 'audit_file]
        +- 'UnresolvedRelation [netsuite_suiteanalytics, 4599_system_note], [], false
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
  	at scala.collection.Iterator.foreach(Iterator.scala:941)
  	at scala.collection.Iterator.foreach$(Iterator.scala:941)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
  	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
  	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
  	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
  	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
  	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
  	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
  	... 16 more
  
17:36:27.188301 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '779f3bd9-1e92-4d0b-a20d-704360cd7056', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000287C97AC160>]}
17:36:27.193314 [error] [Thread-1  ]: 1 of 1 ERROR creating view model dbt.4599_system_note........................... [[31mERROR[0m in 18.44s]
17:36:27.193314 [debug] [Thread-1  ]: Finished running node model.dbt_fivetran_1.4599_system_note
17:36:27.193314 [debug] [MainThread]: Acquiring new databricks connection "master"
17:36:27.198365 [debug] [MainThread]: On master: ROLLBACK
17:36:27.198365 [debug] [MainThread]: Opening a new connection, currently in state init
17:36:28.247346 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
17:36:28.247346 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
17:36:28.247346 [debug] [MainThread]: Spark adapter: NotImplemented: commit
17:36:28.247346 [debug] [MainThread]: On master: ROLLBACK
17:36:28.247346 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
17:36:28.247346 [debug] [MainThread]: On master: Close
17:36:29.278239 [info ] [MainThread]: 
17:36:29.278239 [info ] [MainThread]: Finished running 1 view model in 29.25s.
17:36:29.278239 [debug] [MainThread]: Connection 'master' was properly closed.
17:36:29.278239 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
17:36:29.278239 [debug] [MainThread]: Connection 'list_None_dbt' was properly closed.
17:36:29.278239 [debug] [MainThread]: Connection 'model.dbt_fivetran_1.4599_system_note' was properly closed.
17:36:29.295168 [info ] [MainThread]: 
17:36:29.295168 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
17:36:29.295168 [info ] [MainThread]: 
17:36:29.303160 [error] [MainThread]: [33mRuntime Error in model 4599_system_note (models\example\4599_system_note.sql)[0m
17:36:29.303160 [error] [MainThread]:   org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: netsuite_suiteanalytics.4599_system_note; line 6 pos 34;
17:36:29.303160 [error] [MainThread]:   'GlobalLimit 10
17:36:29.308214 [error] [MainThread]:   +- 'LocalLimit 10
17:36:29.308214 [error] [MainThread]:      +- 'Project ['action, 'audit_file]
17:36:29.308214 [error] [MainThread]:         +- 'UnresolvedRelation [netsuite_suiteanalytics, 4599_system_note], [], false
17:36:29.313228 [error] [MainThread]:   
17:36:29.313228 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1019)
17:36:29.318282 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:759)
17:36:29.318282 [error] [MainThread]:   	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
17:36:29.323841 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
17:36:29.323841 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
17:36:29.323841 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:56)
17:36:29.328385 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:737)
17:36:29.328385 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:722)
17:36:29.328385 [error] [MainThread]:   	at java.security.AccessController.doPrivileged(Native Method)
17:36:29.328385 [error] [MainThread]:   	at javax.security.auth.Subject.doAs(Subject.java:422)
17:36:29.343648 [error] [MainThread]:   	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
17:36:29.343648 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:771)
17:36:29.348237 [error] [MainThread]:   	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
17:36:29.348237 [error] [MainThread]:   	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
17:36:29.353256 [error] [MainThread]:   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
17:36:29.353256 [error] [MainThread]:   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
17:36:29.358358 [error] [MainThread]:   	at java.lang.Thread.run(Thread.java:748)
17:36:29.358358 [error] [MainThread]:   Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: netsuite_suiteanalytics.4599_system_note; line 6 pos 34;
17:36:29.358358 [error] [MainThread]:   'GlobalLimit 10
17:36:29.363372 [error] [MainThread]:   +- 'LocalLimit 10
17:36:29.363372 [error] [MainThread]:      +- 'Project ['action, 'audit_file]
17:36:29.363372 [error] [MainThread]:         +- 'UnresolvedRelation [netsuite_suiteanalytics, 4599_system_note], [], false
17:36:29.368480 [error] [MainThread]:   
17:36:29.368480 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
17:36:29.368480 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
17:36:29.368480 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
17:36:29.368480 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
17:36:29.368480 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
17:36:29.368480 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
17:36:29.377867 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
17:36:29.377867 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
17:36:29.377867 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
17:36:29.377867 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
17:36:29.377867 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
17:36:29.388413 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
17:36:29.388413 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
17:36:29.388413 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
17:36:29.393427 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
17:36:29.393427 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
17:36:29.393427 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
17:36:29.393427 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
17:36:29.398518 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
17:36:29.398518 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
17:36:29.398518 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
17:36:29.403533 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
17:36:29.403533 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
17:36:29.403533 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
17:36:29.403533 [error] [MainThread]:   	at scala.collection.Iterator.foreach(Iterator.scala:941)
17:36:29.408109 [error] [MainThread]:   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
17:36:29.408109 [error] [MainThread]:   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
17:36:29.413127 [error] [MainThread]:   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
17:36:29.413127 [error] [MainThread]:   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
17:36:29.418233 [error] [MainThread]:   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
17:36:29.418233 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
17:36:29.418233 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
17:36:29.418233 [error] [MainThread]:   	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
17:36:29.423789 [error] [MainThread]:   	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
17:36:29.423789 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
17:36:29.423789 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
17:36:29.428383 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
17:36:29.428383 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
17:36:29.433399 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
17:36:29.433399 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
17:36:29.433399 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
17:36:29.438506 [error] [MainThread]:   	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
17:36:29.438506 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
17:36:29.438506 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
17:36:29.438506 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
17:36:29.443522 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
17:36:29.443522 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
17:36:29.448114 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
17:36:29.448114 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
17:36:29.448114 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)
17:36:29.453130 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
17:36:29.453130 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
17:36:29.453130 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
17:36:29.458224 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
17:36:29.458224 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
17:36:29.463244 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
17:36:29.463244 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
17:36:29.468378 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
17:36:29.468378 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
17:36:29.473391 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
17:36:29.473391 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
17:36:29.473391 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
17:36:29.478498 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
17:36:29.478498 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:860)
17:36:29.483511 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
17:36:29.483511 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:842)
17:36:29.483511 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:831)
17:36:29.488089 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:842)
17:36:29.488089 [error] [MainThread]:   	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:896)
17:36:29.488089 [error] [MainThread]:   	... 16 more
17:36:29.493109 [error] [MainThread]:   
17:36:29.493109 [info ] [MainThread]: 
17:36:29.493109 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
17:36:29.503260 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000287C958A580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000287C959DA30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000287C9706820>]}
